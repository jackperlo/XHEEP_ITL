{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oS4veW4RiH5a"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99Dj3g86-u4o"
   },
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkepmwV5h9vK",
    "outputId": "b5201cbe-3a92-4ebf-d0a7-86578d80317f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.15.0\n",
      "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
      "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.1\n",
      "    Uninstalling ml-dtypes-0.4.1:\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.4.1\n",
      "    Uninstalling keras-3.4.1:\n",
      "      Successfully uninstalled keras-3.4.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.0\n",
      "    Uninstalling tensorboard-2.17.0:\n",
      "      Successfully uninstalled tensorboard-2.17.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorstore 0.1.66 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow==2.15.0\n",
    "import os\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import datasets, layers, models, losses\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtJz7wpnMvIn"
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVCidP2XM5Ou"
   },
   "outputs": [],
   "source": [
    "def get_file_size(file_path):\n",
    "    size = os.path.getsize(file_path)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uv81eDa2M-1x"
   },
   "outputs": [],
   "source": [
    "def convert_bytes(size, unit=None):\n",
    "    if unit == \"KB\":\n",
    "        return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes')\n",
    "    elif unit == \"MB\":\n",
    "        return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes')\n",
    "    else:\n",
    "        return print('File size: ' + str(size) + ' bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzND315giZMJ"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnD3f0ad-yRi"
   },
   "source": [
    "\n",
    "Define the keras model for Lenet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JleDRgO2ia_o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import datasets, layers, models, losses\n",
    "import numpy as np\n",
    "@keras.saving.register_keras_serializable(package=\"Lenet\", name=\"Lenet\")\n",
    "class Lenet(tf.keras.Model):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.conv1 = self._make_conv_layer(6, 5, input_shape=[32, 32, 1])\n",
    "    self.conv2 = self._make_conv_layer(16, 5)\n",
    "    self.conv3 = self._make_conv_layer(120, 5, pooling=False, flatten=True)\n",
    "    self.dense1 = layers.Dense(84, activation='tanh')\n",
    "    self.dense2 = layers.Dense(10, activation='softmax')\n",
    "\n",
    "  def call(self, inputs, dense_inputs=tf.zeros((1,120))):\n",
    "    x = self.conv1(inputs)\n",
    "    x = self.conv2(x)\n",
    "    conv_res = self.conv3(x)\n",
    "    dense_out = self.dense2(self.dense1(dense_inputs))\n",
    "    dense1 = self.dense1(conv_res)\n",
    "    out = self.dense2(dense1)\n",
    "    return {'out': out, 'out_dense': dense_out}\n",
    "\n",
    "  def _make_conv_layer(self, filters, kernel_size, input_shape=None, pooling=True, flatten=False):\n",
    "    l = keras.Sequential()\n",
    "    if input_shape is not None:\n",
    "      l.add(layers.Conv2D(filters, kernel_size, activation='tanh', input_shape=input_shape))\n",
    "    else:\n",
    "      l.add(layers.Conv2D(filters, kernel_size, activation='tanh'))\n",
    "    if pooling:\n",
    "      l.add(layers.AveragePooling2D(2))\n",
    "      l.add(layers.Activation('sigmoid'))\n",
    "    if flatten:\n",
    "      l.add(layers.Flatten())\n",
    "    return l\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super().get_config()\n",
    "    config.update({\n",
    "        \"conv1\": keras.saving.serialize_keras_object(self.conv1),\n",
    "        \"conv2\": keras.saving.serialize_keras_object(self.conv2),\n",
    "        \"conv3\": keras.saving.serialize_keras_object(self.conv3),\n",
    "        \"dense1\": keras.saving.serialize_keras_object(self.dense1),\n",
    "        \"dense2\": keras.saving.serialize_keras_object(self.dense2)\n",
    "    })\n",
    "    return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrmbyO5C_Xg-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKfi63YYk-Md"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0Sjt8_j-8N9"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Build the dataset. Images need to be in floating point format, so the values are divided by 255. They are also padded so that height and width are a power of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "AaU3KF9fk_2F",
    "outputId": "b3381e29-2c13-4335-d2b6-10bb8b3976e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "x_train = tf.pad(x_train, [[0, 0], [2, 2], [2, 2]]) / 255\n",
    "x_test = tf.pad(x_test, [[0, 0], [2, 2], [2, 2]]) / 255\n",
    "x_train = tf.expand_dims(x_train, axis=3, name=None)\n",
    "x_test = tf.expand_dims(x_test, axis=3, name=None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print the first 10 images\n",
    "for i in range(1):\n",
    "    plt.imshow(x_train[i], cmap='gray')\n",
    "    plt.title(y_train[i])\n",
    "    plt.show()\n",
    "\n",
    "# Select an image\n",
    "image = x_train[0]\n",
    "\n",
    "# Convert the image to bytes\n",
    "image_bytes = bytes(image)\n",
    "\n",
    "# Convert the bytes to hex\n",
    "image_hex = ' '.join(format(byte, '02x') for byte in image_bytes)\n",
    "\n",
    "#print(image_hex)\n",
    "with open('image_grey.hex', 'w') as f:\n",
    "    f.write(image_hex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ9BgcHb_Qnk"
   },
   "source": [
    "Take some images for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlOt523xlnfQ"
   },
   "outputs": [],
   "source": [
    "x_val = x_train[-2000:,:,:,:]\n",
    "y_val = y_train[-2000:]\n",
    "x_train = x_train[:-2000,:,:,:]\n",
    "y_train = y_train[:-2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQJnxR1A_YDD"
   },
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFUpaCFdlqne",
    "outputId": "86e7b138-10a6-4d90-b50a-b7e76ad65a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lenet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 14, 14, 6)         156       \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (1, 5, 5, 16)             2416      \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (1, 120)                  48120     \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  10164     \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61706 (241.04 KB)\n",
      "Trainable params: 61706 (241.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Lenet()\n",
    "model(x_train[0][None,:,:,:], tf.ones((1,120)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_HSsyyVO9ps"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2HvqmggMoHKN",
    "outputId": "0ad81109-1e9b-419b-ce9d-a167b6d693b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "454/454 [==============================] - 7s 5ms/step - loss: 1.9327 - accuracy: 0.2848 - val_loss: 0.5252 - val_accuracy: 0.8705\n",
      "Epoch 2/5\n",
      "454/454 [==============================] - 2s 4ms/step - loss: 0.4610 - accuracy: 0.8572 - val_loss: 0.2364 - val_accuracy: 0.9335\n",
      "Epoch 3/5\n",
      "454/454 [==============================] - 2s 5ms/step - loss: 0.3208 - accuracy: 0.8997 - val_loss: 0.1986 - val_accuracy: 0.9420\n",
      "Epoch 4/5\n",
      "454/454 [==============================] - 2s 4ms/step - loss: 0.2651 - accuracy: 0.9167 - val_loss: 0.2093 - val_accuracy: 0.9360\n",
      "Epoch 5/5\n",
      "454/454 [==============================] - 2s 5ms/step - loss: 0.2337 - accuracy: 0.9274 - val_loss: 0.1476 - val_accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model.compile(optimizer='adam', loss=losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_val, y_val))\n",
    "model.save('lenet5.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRt8m7PhPSam"
   },
   "source": [
    "Get dimension of .keras trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvNgMfWsPDKG",
    "outputId": "25756061-6c3e-45d4-db76-d28b0e3c5ca0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 771.019 Kilobytes\n"
     ]
    }
   ],
   "source": [
    "convert_bytes(get_file_size(\"lenet5.keras\"), \"KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z-CPkIKl4dYx",
    "outputId": "f6240c78-074c-4cce-e2b7-86ae9548537c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 808ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'out': array([[4.8833364e-04, 8.0063578e-04, 7.0460519e-04, 2.7555346e-01,\n",
       "         6.7719259e-07, 7.1977901e-01, 9.9687782e-08, 1.1022899e-03,\n",
       "         1.2780941e-03, 2.9276474e-04]], dtype=float32),\n",
       " 'out_dense': array([[0.09420101, 0.09051015, 0.11445776, 0.10434542, 0.09869833,\n",
       "         0.11275383, 0.08072697, 0.09569866, 0.10742386, 0.10118403]],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('lenet5.keras')\n",
    "model.predict(x_train[0][None,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "tL_wfaFeUkOf",
    "outputId": "5018e324-1cb9-4539-8a1d-e0670aa2447a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The inference_input_type and inference_output_type must be in ['tf.float32', 'tf.int8', 'tf.uint8'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-15b6f7a2fe15>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_output_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0mtflite_quant_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{layer_name}.tflite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2183\u001b[0m         \u001b[0mInvalid\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m     \"\"\"\n\u001b[0;32m-> 2185\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFLiteConverterV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1790\u001b[0m     )\n\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m     return super(TFLiteFrozenGraphConverterV2, self).convert(\n\u001b[0m\u001b[1;32m   1793\u001b[0m         \u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mInvalid\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \"\"\"\n\u001b[0;32m-> 1354\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m     \u001b[0mconverter_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_base_converter_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[0mconverter_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_validate_inputs\u001b[0;34m(self, graph_def, input_tensors)\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_variable_quantization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m     )\n\u001b[0;32m-> 1238\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inference_input_output_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quant_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_unknown_shapes_allowed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_validate_inference_input_output_types\u001b[0;34m(self, quant_mode)\u001b[0m\n\u001b[1;32m   1167\u001b[0m       ):\n\u001b[1;32m   1168\u001b[0m         \u001b[0mall_types_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"tf.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_types\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0;34m\"The inference_input_type and inference_output_type \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m             \u001b[0;34m\"must be in {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_types_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The inference_input_type and inference_output_type must be in ['tf.float32', 'tf.int8', 'tf.uint8']."
     ]
    }
   ],
   "source": [
    "@tf.function(\n",
    "    input_signature=[tf.TensorSpec(shape=[1, 28, 28, 6], dtype=tf.float32)]\n",
    ")\n",
    "def activation_conv1(x):\n",
    "  out = tf.math.tanh(x)\n",
    "  out = tf.nn.avg_pool(out, ksize=2, strides=2, padding='VALID')\n",
    "  k = tf.math.sigmoid(out)\n",
    "  k = tf.nn.tanh(k)\n",
    "  return k\n",
    "\n",
    "@tf.function(\n",
    "    input_signature=[tf.TensorSpec(shape=[1, 10, 10, 16], dtype=tf.float32)]\n",
    ")\n",
    "def activation_conv2(x):\n",
    "  out = tf.math.tanh(x)\n",
    "  out = tf.nn.avg_pool(out, ksize=2, strides=2, padding='VALID')\n",
    "  k = tf.math.sigmoid(out)\n",
    "  k = tf.nn.tanh(k)\n",
    "  return k\n",
    "\n",
    "@tf.function(\n",
    "    input_signature=[tf.TensorSpec(shape=[1, 1, 1, 120], dtype=tf.float32)]\n",
    ")\n",
    "def activation_conv3(x):\n",
    "  out = tf.math.tanh(x)\n",
    "  k = tf.nn.tanh(out)\n",
    "  return k\n",
    "\n",
    "@tf.function(\n",
    "    input_signature=[tf.TensorSpec(shape=[1, 84], dtype=tf.float32)]\n",
    ")\n",
    "def activation_dense1(x):\n",
    "  out = tf.math.tanh(x)\n",
    "  return out\n",
    "\n",
    "@tf.function(\n",
    "    input_signature=[tf.TensorSpec(shape=[1, 10], dtype=tf.float32)]\n",
    ")\n",
    "def activation_dense2(x):\n",
    "  out = tf.nn.softmax(x)\n",
    "  return out\n",
    "\n",
    "def representative_dataset():\n",
    "  for data in tf.data.Dataset.from_tensor_slices((x_train)).batch(1).take(1000):\n",
    "    yield [data]\n",
    "\n",
    "activations = {\n",
    "    \"activation_conv1\": activation_conv1,\n",
    "    \"activation_conv2\": activation_conv2,\n",
    "    \"activation_conv3\": activation_conv3,\n",
    "    \"activation_dense1\": activation_dense1,\n",
    "    \"activation_dense2\": activation_dense2,\n",
    "}\n",
    "\n",
    "for layer_name, activation in activations.items():\n",
    "  converter = tf.lite.TFLiteConverter.from_concrete_functions([activation.get_concrete_function()])\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT] # Recommended optimization\n",
    "  converter.representative_dataset = representative_dataset\n",
    " # Specify that the quantized model should only support int8 operations\n",
    "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "  converter.inference_input_type = tf.int32\n",
    "  converter.inference_output_type = tf.int8\n",
    "\n",
    "  tflite_quant_model = converter.convert()\n",
    "\n",
    "  with open(f'{layer_name}.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHbj0UrLVM5g",
    "outputId": "a93c96fe-aacb-4fd9-eedc-bbafdcfef982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== lenet5_edited.tflite ===\n",
      "\n",
      "Your TFLite model has '1' subgraph(s). In the subgraph description below,\n",
      "T# represents the Tensor numbers. For example, in Subgraph#0, the FULLY_CONNECTED op takes\n",
      "tensor #1 and tensor #14 and tensor #12 as input and produces tensor #15 as output.\n",
      "\n",
      "Subgraph#0 main(T#0, T#1) -> [T#33, T#18]\n",
      "  Op#0 FULLY_CONNECTED(T#1, T#14, T#12[-815, -67, -347, 943, -314, ...]) -> [T#15]\n",
      "  Op#1 TANH(T#15) -> [T#16]\n",
      "  Op#2 FULLY_CONNECTED(T#16, T#11, T#9[-79, -1298, 2288, -174, 493, ...]) -> [T#17]\n",
      "  Op#3 SOFTMAX(T#17) -> [T#18]\n",
      "  Op#4 CONV_2D(T#0, T#8, T#7[-28847, 16208, -15145, -21816, 15629, ...]) -> [T#19]\n",
      "  Op#5 TANH(T#19) -> [T#20]\n",
      "  Op#6 AVERAGE_POOL_2D(T#20) -> [T#21]\n",
      "  Op#7 LOGISTIC(T#21) -> [T#22]\n",
      "  Op#8 CONV_2D(T#22, T#6, T#5[271, 453, -12072, -13127, -44, ...]) -> [T#23]\n",
      "  Op#9 TANH(T#23) -> [T#24]\n",
      "  Op#10 AVERAGE_POOL_2D(T#24) -> [T#25]\n",
      "  Op#11 LOGISTIC(T#25) -> [T#26]\n",
      "  Op#12 CONV_2D(T#26, T#4, T#3[-103, -1609, -72, -528, -311, ...]) -> [T#27]\n",
      "  Op#13 TANH(T#27) -> [T#28]\n",
      "  Op#14 RESHAPE(T#28, T#2[-1, 120]) -> [T#29]\n",
      "  Op#15 FULLY_CONNECTED(T#29, T#14, T#13[-735, -60, -314, 851, -284, ...]) -> [T#30]\n",
      "  Op#16 TANH(T#30) -> [T#31]\n",
      "  Op#17 FULLY_CONNECTED(T#31, T#11, T#10[-79, -1298, 2288, -174, 493, ...]) -> [T#32]\n",
      "  Op#18 SOFTMAX(T#32) -> [T#33]\n",
      "\n",
      "Tensors of Subgraph#0\n",
      "  T#0(serving_default_args_0:0) shape_signature:[-1, 32, 32, 1], type:INT8\n",
      "  T#1(serving_default_args_1:0) shape_signature:[-1, 120], type:INT8\n",
      "  T#2(lenet_21/sequential_63/flatten_20/Const) shape:[2], type:INT32 RO 8 bytes, buffer: 3, data:[-1, 120]\n",
      "  T#3(lenet_21/sequential_63/conv2d_63/BiasAdd/ReadVariableOp) shape:[120], type:INT32 RO 480 bytes, buffer: 4, data:[-103, -1609, -72, -528, -311, ...]\n",
      "  T#4(lenet_21/sequential_63/conv2d_63/Conv2D) shape:[120, 5, 5, 16], type:INT8 RO 48000 bytes, buffer: 5, data:[., ., ., ., ., ...]\n",
      "  T#5(lenet_21/sequential_62/conv2d_62/BiasAdd/ReadVariableOp) shape:[16], type:INT32 RO 64 bytes, buffer: 6, data:[271, 453, -12072, -13127, -44, ...]\n",
      "  T#6(lenet_21/sequential_62/conv2d_62/Conv2D) shape:[16, 5, 5, 6], type:INT8 RO 2400 bytes, buffer: 7, data:[., \n",
      ", ., ., ., ...]\n",
      "  T#7(lenet_21/sequential_61/conv2d_61/BiasAdd/ReadVariableOp) shape:[6], type:INT32 RO 24 bytes, buffer: 8, data:[-28847, 16208, -15145, -21816, 15629, ...]\n",
      "  T#8(lenet_21/sequential_61/conv2d_61/Conv2D) shape:[6, 5, 5, 1], type:INT8 RO 150 bytes, buffer: 9, data:[., ., ., ., -, ...]\n",
      "  T#9(lenet_21/dense_41/BiasAdd/ReadVariableOp) shape:[10], type:INT32 RO 40 bytes, buffer: 10, data:[-79, -1298, 2288, -174, 493, ...]\n",
      "  T#10(lenet_21/dense_41/BiasAdd/ReadVariableOp1) shape:[10], type:INT32 RO 40 bytes, buffer: 10, data:[-79, -1298, 2288, -174, 493, ...]\n",
      "  T#11(lenet_21/dense_41/MatMul_1) shape:[10, 84], type:INT8 RO 840 bytes, buffer: 12, data:[3, ., ., ., ., ...]\n",
      "  T#12(lenet_21/dense_40/BiasAdd/ReadVariableOp) shape:[84], type:INT32 RO 336 bytes, buffer: 13, data:[-815, -67, -347, 943, -314, ...]\n",
      "  T#13(lenet_21/dense_40/BiasAdd/ReadVariableOp1) shape:[84], type:INT32 RO 336 bytes, buffer: 14, data:[-735, -60, -314, 851, -284, ...]\n",
      "  T#14(lenet_21/dense_40/MatMul_1) shape:[84, 120], type:INT8 RO 10080 bytes, buffer: 15, data:[\n",
      ", ., ., ., ., ...]\n",
      "  T#15(lenet_21/dense_40/MatMul;lenet_21/dense_40/BiasAdd) shape_signature:[-1, 84], type:INT8\n",
      "  T#16(lenet_21/dense_40/Tanh) shape_signature:[-1, 84], type:INT8\n",
      "  T#17(lenet_21/dense_41/MatMul;lenet_21/dense_41/BiasAdd) shape_signature:[-1, 10], type:INT8\n",
      "  T#18(StatefulPartitionedCall:1) shape_signature:[-1, 10], type:INT8\n",
      "  T#19(lenet_21/sequential_61/conv2d_61/BiasAdd;lenet_21/sequential_61/conv2d_61/Conv2D;lenet_21/sequential_61/conv2d_61/BiasAdd/ReadVariableOp) shape_signature:[-1, 28, 28, 6], type:INT8\n",
      "  T#20(lenet_21/sequential_61/conv2d_61/Tanh) shape_signature:[-1, 28, 28, 6], type:INT8\n",
      "  T#21(lenet_21/sequential_61/average_pooling2d_41/AvgPool) shape_signature:[-1, 14, 14, 6], type:INT8\n",
      "  T#22(lenet_21/sequential_61/activation_40/Sigmoid) shape_signature:[-1, 14, 14, 6], type:INT8\n",
      "  T#23(lenet_21/sequential_62/conv2d_62/BiasAdd;lenet_21/sequential_62/conv2d_62/Conv2D;lenet_21/sequential_62/conv2d_62/BiasAdd/ReadVariableOp) shape_signature:[-1, 10, 10, 16], type:INT8\n",
      "  T#24(lenet_21/sequential_62/conv2d_62/Tanh) shape_signature:[-1, 10, 10, 16], type:INT8\n",
      "  T#25(lenet_21/sequential_62/average_pooling2d_42/AvgPool) shape_signature:[-1, 5, 5, 16], type:INT8\n",
      "  T#26(lenet_21/sequential_62/activation_41/Sigmoid) shape_signature:[-1, 5, 5, 16], type:INT8\n",
      "  T#27(lenet_21/sequential_63/conv2d_63/BiasAdd;lenet_21/sequential_63/conv2d_63/Conv2D;lenet_21/sequential_63/conv2d_63/BiasAdd/ReadVariableOp) shape_signature:[-1, 1, 1, 120], type:INT8\n",
      "  T#28(lenet_21/sequential_63/conv2d_63/Tanh) shape_signature:[-1, 1, 1, 120], type:INT8\n",
      "  T#29(lenet_21/sequential_63/flatten_20/Reshape) shape_signature:[-1, 120], type:INT8\n",
      "  T#30(lenet_21/dense_40/MatMul_1;lenet_21/dense_40/BiasAdd_1) shape_signature:[-1, 84], type:INT8\n",
      "  T#31(lenet_21/dense_40/Tanh_1) shape_signature:[-1, 84], type:INT8\n",
      "  T#32(lenet_21/dense_41/MatMul_1;lenet_21/dense_41/BiasAdd_1) shape_signature:[-1, 10], type:INT8\n",
      "  T#33(StatefulPartitionedCall:0) shape_signature:[-1, 10], type:INT8\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Your TFLite model has '1' signature_def(s).\n",
      "\n",
      "Signature#0 key: 'serving_default'\n",
      "- Subgraph: Subgraph#0\n",
      "- Inputs: \n",
      "    'args_0' : T#0\n",
      "    'args_1' : T#1\n",
      "- Outputs: \n",
      "    'out' : T#33\n",
      "    'out_dense' : T#18\n",
      "\n",
      "---------------------------------------------------------------\n",
      "              Model size:      73272 bytes\n",
      "    Non-data buffer size:      10410 bytes (14.21 %)\n",
      "  Total data buffer size:      62862 bytes (85.79 %)\n",
      "    (Zero value buffers):          0 bytes (00.00 %)\n",
      "\n",
      "* Buffers of TFLite model are mostly used for constant tensors.\n",
      "  And zero value buffers are buffers filled with zeros.\n",
      "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
      "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'lenet_21/sequential_61/activation_40/Sigmoid',\n",
       " 'index': 22,\n",
       " 'shape': array([ 1, 14, 14,  6], dtype=int32),\n",
       " 'shape_signature': array([-1, 14, 14,  6], dtype=int32),\n",
       " 'dtype': numpy.int8,\n",
       " 'quantization': (0.00390625, -128),\n",
       " 'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "  'zero_points': array([-128], dtype=int32),\n",
       "  'quantized_dimension': 0},\n",
       " 'sparsity_parameters': {}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.lite.experimental.Analyzer.analyze(model_path=\"lenet5_edited.tflite\")\n",
    "interpreter = tf.lite.Interpreter(model_path=\"lenet5_edited.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "# Get names of input and output\n",
    "interpreter.get_tensor_details()[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUhVTH0L_8qt"
   },
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "XrHuyctGq_kE",
    "outputId": "42c54138-8798-41f2-ac11-5893cde1061f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Lenet' object has no attribute 'conv_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4806a5fca210>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_output_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtflite_quant_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lenet5_mnist.tflite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         \u001b[0mInvalid\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     \"\"\"\n\u001b[0;32m-> 1601\u001b[0;31m     \u001b[0msaved_model_convert_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_as_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1580\u001b[0m       )\n\u001b[1;32m   1581\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         return super(TFLiteKerasModelConverterV2, self).convert(\n\u001b[0m\u001b[1;32m   1583\u001b[0m             \u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     )\n\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m     return self._optimize_tflite_model(\n\u001b[0m\u001b[1;32m   1379\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quant_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_io\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_new_quantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_optimize_tflite_model\u001b[0;34m(self, model, quant_mode, quant_io)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0mq_allow_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allow_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mq_variable_quantization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_mlir_variable_quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m         model = self._quantize(\n\u001b[0m\u001b[1;32m   1038\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0mq_in_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_quantize\u001b[0;34m(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)\u001b[0m\n\u001b[1;32m    733\u001b[0m     )\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_calibrate_only\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_new_quantizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m       calibrated = calibrate_quantize.calibrate(\n\u001b[0m\u001b[1;32m    736\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_gen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m       )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36mcalibrate\u001b[0;34m(self, dataset_gen)\u001b[0m\n\u001b[1;32m    252\u001b[0m       \u001b[0mdataset_gen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mgenerates\u001b[0m \u001b[0mcalibration\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \"\"\"\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalibrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36m_feed_tensors\u001b[0;34m(self, dataset_gen, resize_input)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0minitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4806a5fca210>\u001b[0m in \u001b[0;36mrepresentative_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrepresentative_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Lenet' object has no attribute 'conv_out'"
     ]
    }
   ],
   "source": [
    "# Tensorflow Lite needs a representative dataset in order to determine the correct quantization parameters\n",
    "def representative_dataset():\n",
    "  for data in tf.data.Dataset.from_tensor_slices((x_train)).batch(1).take(1000):\n",
    "    yield [data]\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] # Recommended optimization\n",
    "converter.representative_dataset = representative_dataset\n",
    "# Specify that the quantized model should only support int8 operations\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "with open('lenet5_mnist.tflite', 'wb') as f:\n",
    "  f.write(tflite_quant_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTyrNd-IQYw_"
   },
   "source": [
    "Get dimension of .tflite trained and converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q312pmfTQe2h",
    "outputId": "f633a96c-fbed-4a63-f473-d0ee06bd478c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 69.805 Kilobytes\n"
     ]
    }
   ],
   "source": [
    "convert_bytes(get_file_size(\"lenet5_mnist.tflite\"), \"KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cssQmP4TLMX"
   },
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiOt-uRdAc6i"
   },
   "source": [
    "### TF Lite Analyzer\n",
    "Analyze the model to get details about tensors and operations used in the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skPAk_kCfBLV",
    "outputId": "28f9abc4-01c6-4e58-b8a0-eaa7f10579c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== activation_conv1_edit.tflite ===\n",
      "\n",
      "Your TFLite model has '1' subgraph(s). In the subgraph description below,\n",
      "T# represents the Tensor numbers. For example, in Subgraph#0, the TANH op takes\n",
      "tensor #0 as input and produces tensor #1 as output.\n",
      "\n",
      "Subgraph#0 main(T#0) -> [T#3]\n",
      "  Op#0 TANH(T#0) -> [T#1]\n",
      "  Op#1 AVERAGE_POOL_2D(T#1) -> [T#2]\n",
      "  Op#2 LOGISTIC(T#2) -> [T#3]\n",
      "\n",
      "Tensors of Subgraph#0\n",
      "  T#0(x) shape:[1, 28, 28, 6], type:INT8\n",
      "  T#1(Tanh) shape:[1, 28, 28, 6], type:INT8\n",
      "  T#2(AvgPool) shape:[1, 14, 14, 6], type:INT8\n",
      "  T#3(Identity) shape:[1, 14, 14, 6], type:INT8\n",
      "\n",
      "---------------------------------------------------------------\n",
      "              Model size:       1144 bytes\n",
      "    Non-data buffer size:       1040 bytes (90.91 %)\n",
      "  Total data buffer size:        104 bytes (09.09 %)\n",
      "    (Zero value buffers):          0 bytes (00.00 %)\n",
      "\n",
      "* Buffers of TFLite model are mostly used for constant tensors.\n",
      "  And zero value buffers are buffers filled with zeros.\n",
      "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
      "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'x',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 28, 28,  6], dtype=int32),\n",
       "  'shape_signature': array([ 1, 28, 28,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.05561614781618118, -12),\n",
       "  'quantization_parameters': {'scales': array([0.05561615], dtype=float32),\n",
       "   'zero_points': array([-12], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'Tanh',\n",
       "  'index': 1,\n",
       "  'shape': array([ 1, 28, 28,  6], dtype=int32),\n",
       "  'shape_signature': array([ 1, 28, 28,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'AvgPool',\n",
       "  'index': 2,\n",
       "  'shape': array([ 1, 14, 14,  6], dtype=int32),\n",
       "  'shape_signature': array([ 1, 14, 14,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'Identity',\n",
       "  'index': 3,\n",
       "  'shape': array([ 1, 14, 14,  6], dtype=int32),\n",
       "  'shape_signature': array([ 1, 14, 14,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.00390625, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.lite.experimental.Analyzer.analyze(\"activation_conv1_edit.tflite\")\n",
    "interpreter = tf.lite.Interpreter(model_path=\"activation_conv1_edit.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "# Get names of input and output\n",
    "interpreter.get_tensor_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lec9VYGXD0w0"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTuveg1IzdQL"
   },
   "source": [
    "###Netron\n",
    "As an alternative https://netron.app could be exploited to get a more user-friendly and schematic view of the .tflite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BOrBjVtA8Rp"
   },
   "source": [
    "Build the Tensorflow Lite interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8Q8W9jos5Pu",
    "outputId": "47bdc337-95fe-4cfe-a723-d8f641078528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'serving_default': {'inputs': ['input_1'], 'outputs': ['output_1']}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter('lenet5_mnist.tflite', experimental_preserve_all_tensors=True)\n",
    "interpreter.allocate_tensors()\n",
    "sig = interpreter.get_signature_runner()\n",
    "# Get names of input and output\n",
    "interpreter.get_signature_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2m9JzLhZN88"
   },
   "source": [
    "###Output Tensor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FGgEGUrXnn8"
   },
   "source": [
    "Get the output tensor's details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dT-RuIIvEmb",
    "outputId": "cd545428-f546-421f-df40-1788b0f633d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'serving_default': {'inputs': ['args_0', 'args_1'],\n",
       "  'outputs': ['out', 'out_dense']}}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_signature_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8RDwX23XD22",
    "outputId": "adf1eb04-bd60-4835-8b06-37f5c477e882"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_args_0:0',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 32, 32,  1], dtype=int32),\n",
       "  'shape_signature': array([-1, 32, 32,  1], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.003921568859368563, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'serving_default_args_1:0',\n",
       "  'index': 1,\n",
       "  'shape': array([  1, 120], dtype=int32),\n",
       "  'shape_signature': array([ -1, 120], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.006880663800984621, 3),\n",
       "  'quantization_parameters': {'scales': array([0.00688066], dtype=float32),\n",
       "   'zero_points': array([3], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NC-KxL1OYDkw",
    "outputId": "05272a6d-1c6e-4265-b5f2-84f3f25f0415"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -73],\n",
       "        [ -33],\n",
       "        [  25],\n",
       "        [  26],\n",
       "        [  45]],\n",
       "\n",
       "       [[ -28],\n",
       "        [  62],\n",
       "        [  82],\n",
       "        [ 118],\n",
       "        [  85]],\n",
       "\n",
       "       [[   3],\n",
       "        [  55],\n",
       "        [ 127],\n",
       "        [ 117],\n",
       "        [  76]],\n",
       "\n",
       "       [[ -66],\n",
       "        [  56],\n",
       "        [  99],\n",
       "        [  39],\n",
       "        [   0]],\n",
       "\n",
       "       [[-108],\n",
       "        [ -33],\n",
       "        [ -67],\n",
       "        [ -96],\n",
       "        [-109]]], dtype=int8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer_kernel = interpreter.get_tensor(11).astype(np.int8)\n",
    "#import struct\n",
    "#first_layer_kernel[0,:,:,0].tobytes().hex()\n",
    "first_layer_kernel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v94lRM6vHiH5",
    "outputId": "9a7013ea-3b14-4d3a-f370-bf74aee55ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== lenet5_edited.tflite ===\n",
      "\n",
      "Your TFLite model has '1' subgraph(s). In the subgraph description below,\n",
      "T# represents the Tensor numbers. For example, in Subgraph#0, the FULLY_CONNECTED op takes\n",
      "tensor #1 and tensor #14 and tensor #12 as input and produces tensor #15 as output.\n",
      "\n",
      "Subgraph#0 main(T#0, T#1) -> [T#33, T#18]\n",
      "  Op#0 FULLY_CONNECTED(T#1, T#14, T#12[-815, -67, -347, 943, -314, ...]) -> [T#15]\n",
      "  Op#1 TANH(T#15) -> [T#16]\n",
      "  Op#2 FULLY_CONNECTED(T#16, T#11, T#9[-79, -1298, 2288, -174, 493, ...]) -> [T#17]\n",
      "  Op#3 SOFTMAX(T#17) -> [T#18]\n",
      "  Op#4 CONV_2D(T#0, T#8, T#7[-28847, 16208, -15145, -21816, 15629, ...]) -> [T#19]\n",
      "  Op#5 TANH(T#19) -> [T#20]\n",
      "  Op#6 AVERAGE_POOL_2D(T#20) -> [T#21]\n",
      "  Op#7 LOGISTIC(T#21) -> [T#22]\n",
      "  Op#8 CONV_2D(T#22, T#6, T#5[271, 453, -12072, -13127, -44, ...]) -> [T#23]\n",
      "  Op#9 TANH(T#23) -> [T#24]\n",
      "  Op#10 AVERAGE_POOL_2D(T#24) -> [T#25]\n",
      "  Op#11 LOGISTIC(T#25) -> [T#26]\n",
      "  Op#12 CONV_2D(T#26, T#4, T#3[-103, -1609, -72, -528, -311, ...]) -> [T#27]\n",
      "  Op#13 TANH(T#27) -> [T#28]\n",
      "  Op#14 RESHAPE(T#28, T#2[-1, 120]) -> [T#29]\n",
      "  Op#15 FULLY_CONNECTED(T#29, T#14, T#13[-735, -60, -314, 851, -284, ...]) -> [T#30]\n",
      "  Op#16 TANH(T#30) -> [T#31]\n",
      "  Op#17 FULLY_CONNECTED(T#31, T#11, T#10[-79, -1298, 2288, -174, 493, ...]) -> [T#32]\n",
      "  Op#18 SOFTMAX(T#32) -> [T#33]\n",
      "\n",
      "Tensors of Subgraph#0\n",
      "  T#0(serving_default_args_0:0) shape_signature:[-1, 32, 32, 1], type:INT8\n",
      "  T#1(serving_default_args_1:0) shape_signature:[-1, 120], type:INT8\n",
      "  T#2(lenet_21/sequential_63/flatten_20/Const) shape:[2], type:INT32 RO 8 bytes, buffer: 3, data:[-1, 120]\n",
      "  T#3(lenet_21/sequential_63/conv2d_63/BiasAdd/ReadVariableOp) shape:[120], type:INT32 RO 480 bytes, buffer: 4, data:[-103, -1609, -72, -528, -311, ...]\n",
      "  T#4(lenet_21/sequential_63/conv2d_63/Conv2D) shape:[120, 5, 5, 16], type:INT8 RO 48000 bytes, buffer: 5, data:[., ., ., ., ., ...]\n",
      "  T#5(lenet_21/sequential_62/conv2d_62/BiasAdd/ReadVariableOp) shape:[16], type:INT32 RO 64 bytes, buffer: 6, data:[271, 453, -12072, -13127, -44, ...]\n",
      "  T#6(lenet_21/sequential_62/conv2d_62/Conv2D) shape:[16, 5, 5, 6], type:INT8 RO 2400 bytes, buffer: 7, data:[., \n",
      ", ., ., ., ...]\n",
      "  T#7(lenet_21/sequential_61/conv2d_61/BiasAdd/ReadVariableOp) shape:[6], type:INT32 RO 24 bytes, buffer: 8, data:[-28847, 16208, -15145, -21816, 15629, ...]\n",
      "  T#8(lenet_21/sequential_61/conv2d_61/Conv2D) shape:[6, 5, 5, 1], type:INT8 RO 150 bytes, buffer: 9, data:[., ., ., ., -, ...]\n",
      "  T#9(lenet_21/dense_41/BiasAdd/ReadVariableOp) shape:[10], type:INT32 RO 40 bytes, buffer: 10, data:[-79, -1298, 2288, -174, 493, ...]\n",
      "  T#10(lenet_21/dense_41/BiasAdd/ReadVariableOp1) shape:[10], type:INT32 RO 40 bytes, buffer: 10, data:[-79, -1298, 2288, -174, 493, ...]\n",
      "  T#11(lenet_21/dense_41/MatMul_1) shape:[10, 84], type:INT8 RO 840 bytes, buffer: 12, data:[3, ., ., ., ., ...]\n",
      "  T#12(lenet_21/dense_40/BiasAdd/ReadVariableOp) shape:[84], type:INT32 RO 336 bytes, buffer: 13, data:[-815, -67, -347, 943, -314, ...]\n",
      "  T#13(lenet_21/dense_40/BiasAdd/ReadVariableOp1) shape:[84], type:INT32 RO 336 bytes, buffer: 14, data:[-735, -60, -314, 851, -284, ...]\n",
      "  T#14(lenet_21/dense_40/MatMul_1) shape:[84, 120], type:INT8 RO 10080 bytes, buffer: 15, data:[\n",
      ", ., ., ., ., ...]\n",
      "  T#15(lenet_21/dense_40/MatMul;lenet_21/dense_40/BiasAdd) shape_signature:[-1, 84], type:INT8\n",
      "  T#16(lenet_21/dense_40/Tanh) shape_signature:[-1, 84], type:INT8\n",
      "  T#17(lenet_21/dense_41/MatMul;lenet_21/dense_41/BiasAdd) shape_signature:[-1, 10], type:INT8\n",
      "  T#18(StatefulPartitionedCall:1) shape_signature:[-1, 10], type:INT8\n",
      "  T#19(lenet_21/sequential_61/conv2d_61/BiasAdd;lenet_21/sequential_61/conv2d_61/Conv2D;lenet_21/sequential_61/conv2d_61/BiasAdd/ReadVariableOp) shape_signature:[-1, 28, 28, 6], type:INT8\n",
      "  T#20(lenet_21/sequential_61/conv2d_61/Tanh) shape_signature:[-1, 28, 28, 6], type:INT8\n",
      "  T#21(lenet_21/sequential_61/average_pooling2d_41/AvgPool) shape_signature:[-1, 14, 14, 6], type:INT8\n",
      "  T#22(lenet_21/sequential_61/activation_40/Sigmoid) shape_signature:[-1, 14, 14, 6], type:INT8\n",
      "  T#23(lenet_21/sequential_62/conv2d_62/BiasAdd;lenet_21/sequential_62/conv2d_62/Conv2D;lenet_21/sequential_62/conv2d_62/BiasAdd/ReadVariableOp) shape_signature:[-1, 10, 10, 16], type:INT8\n",
      "  T#24(lenet_21/sequential_62/conv2d_62/Tanh) shape_signature:[-1, 10, 10, 16], type:INT8\n",
      "  T#25(lenet_21/sequential_62/average_pooling2d_42/AvgPool) shape_signature:[-1, 5, 5, 16], type:INT8\n",
      "  T#26(lenet_21/sequential_62/activation_41/Sigmoid) shape_signature:[-1, 5, 5, 16], type:INT8\n",
      "  T#27(lenet_21/sequential_63/conv2d_63/BiasAdd;lenet_21/sequential_63/conv2d_63/Conv2D;lenet_21/sequential_63/conv2d_63/BiasAdd/ReadVariableOp) shape_signature:[-1, 1, 1, 120], type:INT8\n",
      "  T#28(lenet_21/sequential_63/conv2d_63/Tanh) shape_signature:[-1, 1, 1, 120], type:INT8\n",
      "  T#29(lenet_21/sequential_63/flatten_20/Reshape) shape_signature:[-1, 120], type:INT8\n",
      "  T#30(lenet_21/dense_40/MatMul_1;lenet_21/dense_40/BiasAdd_1) shape_signature:[-1, 84], type:INT8\n",
      "  T#31(lenet_21/dense_40/Tanh_1) shape_signature:[-1, 84], type:INT8\n",
      "  T#32(lenet_21/dense_41/MatMul_1;lenet_21/dense_41/BiasAdd_1) shape_signature:[-1, 10], type:INT8\n",
      "  T#33(StatefulPartitionedCall:0) shape_signature:[-1, 10], type:INT8\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Your TFLite model has '1' signature_def(s).\n",
      "\n",
      "Signature#0 key: 'serving_default'\n",
      "- Subgraph: Subgraph#0\n",
      "- Inputs: \n",
      "    'args_0' : T#0\n",
      "    'args_1' : T#1\n",
      "- Outputs: \n",
      "    'out' : T#33\n",
      "    'out_dense' : T#18\n",
      "\n",
      "---------------------------------------------------------------\n",
      "              Model size:      73272 bytes\n",
      "    Non-data buffer size:      10410 bytes (14.21 %)\n",
      "  Total data buffer size:      62862 bytes (85.79 %)\n",
      "    (Zero value buffers):          0 bytes (00.00 %)\n",
      "\n",
      "* Buffers of TFLite model are mostly used for constant tensors.\n",
      "  And zero value buffers are buffers filled with zeros.\n",
      "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
      "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
      "\n",
      "=== lenet5_mnist.tflite ===\n",
      "\n",
      "Your TFLite model has '1' subgraph(s). In the subgraph description below,\n",
      "T# represents the Tensor numbers. For example, in Subgraph#0, the CONV_2D op takes\n",
      "tensor #0 and tensor #11 and tensor #10 as input and produces tensor #12 as output.\n",
      "\n",
      "Subgraph#0 main(T#0) -> [T#26]\n",
      "  Op#0 CONV_2D(T#0, T#11, T#10[-28847, 16208, -15145, -21816, 15629, ...]) -> [T#12]\n",
      "  Op#1 TANH(T#12) -> [T#13]\n",
      "  Op#2 AVERAGE_POOL_2D(T#13) -> [T#14]\n",
      "  Op#3 LOGISTIC(T#14) -> [T#15]\n",
      "  Op#4 CONV_2D(T#15, T#9, T#8[271, 453, -12072, -13127, -44, ...]) -> [T#16]\n",
      "  Op#5 TANH(T#16) -> [T#17]\n",
      "  Op#6 AVERAGE_POOL_2D(T#17) -> [T#18]\n",
      "  Op#7 LOGISTIC(T#18) -> [T#19]\n",
      "  Op#8 CONV_2D(T#19, T#7, T#6[-103, -1609, -72, -528, -311, ...]) -> [T#20]\n",
      "  Op#9 TANH(T#20) -> [T#21]\n",
      "  Op#10 RESHAPE(T#21, T#1[-1, 120]) -> [T#22]\n",
      "  Op#11 FULLY_CONNECTED(T#22, T#5, T#4[-735, -60, -314, 851, -284, ...]) -> [T#23]\n",
      "  Op#12 TANH(T#23) -> [T#24]\n",
      "  Op#13 FULLY_CONNECTED(T#24, T#3, T#2[-79, -1298, 2288, -174, 493, ...]) -> [T#25]\n",
      "  Op#14 SOFTMAX(T#25) -> [T#26]\n",
      "\n",
      "Tensors of Subgraph#0\n",
      "  T#0(serving_default_input_1:0) shape_signature:[-1, 32, 32, 1], type:INT8\n",
      "  T#1(lenet/sequential_2/flatten/Const) shape:[2], type:INT32 RO 8 bytes, buffer: 2, data:[-1, 120]\n",
      "  T#2(lenet/dense_1/BiasAdd/ReadVariableOp) shape:[10], type:INT32 RO 40 bytes, buffer: 3, data:[-79, -1298, 2288, -174, 493, ...]\n",
      "  T#3(lenet/dense_1/MatMul) shape:[10, 84], type:INT8 RO 840 bytes, buffer: 4, data:[3, ., ., ., ., ...]\n",
      "  T#4(lenet/dense/BiasAdd/ReadVariableOp) shape:[84], type:INT32 RO 336 bytes, buffer: 5, data:[-735, -60, -314, 851, -284, ...]\n",
      "  T#5(lenet/dense/MatMul) shape:[84, 120], type:INT8 RO 10080 bytes, buffer: 6, data:[\n",
      ", ., ., ., ., ...]\n",
      "  T#6(lenet/sequential_2/conv2d_2/BiasAdd/ReadVariableOp) shape:[120], type:INT32 RO 480 bytes, buffer: 7, data:[-103, -1609, -72, -528, -311, ...]\n",
      "  T#7(lenet/sequential_2/conv2d_2/Conv2D) shape:[120, 5, 5, 16], type:INT8 RO 48000 bytes, buffer: 8, data:[., ., ., ., ., ...]\n",
      "  T#8(lenet/sequential_1/conv2d_1/BiasAdd/ReadVariableOp) shape:[16], type:INT32 RO 64 bytes, buffer: 9, data:[271, 453, -12072, -13127, -44, ...]\n",
      "  T#9(lenet/sequential_1/conv2d_1/Conv2D) shape:[16, 5, 5, 6], type:INT8 RO 2400 bytes, buffer: 10, data:[., \n",
      ", ., ., ., ...]\n",
      "  T#10(lenet/sequential/conv2d/BiasAdd/ReadVariableOp) shape:[6], type:INT32 RO 24 bytes, buffer: 11, data:[-28847, 16208, -15145, -21816, 15629, ...]\n",
      "  T#11(lenet/sequential/conv2d/Conv2D) shape:[6, 5, 5, 1], type:INT8 RO 150 bytes, buffer: 12, data:[., ., ., ., -, ...]\n",
      "  T#12(lenet/sequential/conv2d/BiasAdd;lenet/sequential/conv2d/Conv2D;lenet/sequential/conv2d/BiasAdd/ReadVariableOp) shape_signature:[-1, 28, 28, 6], type:INT8\n",
      "  T#13(lenet/sequential/conv2d/Tanh) shape_signature:[-1, 28, 28, 6], type:INT8\n",
      "  T#14(lenet/sequential/average_pooling2d/AvgPool) shape_signature:[-1, 14, 14, 6], type:INT8\n",
      "  T#15(lenet/sequential/activation/Sigmoid) shape_signature:[-1, 14, 14, 6], type:INT8\n",
      "  T#16(lenet/sequential_1/conv2d_1/BiasAdd;lenet/sequential_1/conv2d_1/Conv2D;lenet/sequential_1/conv2d_1/BiasAdd/ReadVariableOp) shape_signature:[-1, 10, 10, 16], type:INT8\n",
      "  T#17(lenet/sequential_1/conv2d_1/Tanh) shape_signature:[-1, 10, 10, 16], type:INT8\n",
      "  T#18(lenet/sequential_1/average_pooling2d_1/AvgPool) shape_signature:[-1, 5, 5, 16], type:INT8\n",
      "  T#19(lenet/sequential_1/activation_1/Sigmoid) shape_signature:[-1, 5, 5, 16], type:INT8\n",
      "  T#20(lenet/sequential_2/conv2d_2/BiasAdd;lenet/sequential_2/conv2d_2/Conv2D;lenet/sequential_2/conv2d_2/BiasAdd/ReadVariableOp) shape_signature:[-1, 1, 1, 120], type:INT8\n",
      "  T#21(lenet/sequential_2/conv2d_2/Tanh) shape_signature:[-1, 1, 1, 120], type:INT8\n",
      "  T#22(lenet/sequential_2/flatten/Reshape) shape_signature:[-1, 120], type:INT8\n",
      "  T#23(lenet/dense/MatMul;lenet/dense/BiasAdd) shape_signature:[-1, 84], type:INT8\n",
      "  T#24(lenet/dense/Tanh) shape_signature:[-1, 84], type:INT8\n",
      "  T#25(lenet/dense_1/MatMul;lenet/dense_1/BiasAdd) shape_signature:[-1, 10], type:INT8\n",
      "  T#26(StatefulPartitionedCall:0) shape_signature:[-1, 10], type:INT8\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Your TFLite model has '1' signature_def(s).\n",
      "\n",
      "Signature#0 key: 'serving_default'\n",
      "- Subgraph: Subgraph#0\n",
      "- Inputs: \n",
      "    'input_1' : T#0\n",
      "- Outputs: \n",
      "    'output_1' : T#26\n",
      "\n",
      "---------------------------------------------------------------\n",
      "              Model size:      71480 bytes\n",
      "    Non-data buffer size:       8954 bytes (12.53 %)\n",
      "  Total data buffer size:      62526 bytes (87.47 %)\n",
      "    (Zero value buffers):          0 bytes (00.00 %)\n",
      "\n",
      "* Buffers of TFLite model are mostly used for constant tensors.\n",
      "  And zero value buffers are buffers filled with zeros.\n",
      "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
      "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_input_1:0',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 32, 32,  1], dtype=int32),\n",
       "  'shape_signature': array([-1, 32, 32,  1], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.003921568859368563, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_2/flatten/Const',\n",
       "  'index': 1,\n",
       "  'shape': array([2], dtype=int32),\n",
       "  'shape_signature': array([2], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/dense_1/BiasAdd/ReadVariableOp',\n",
       "  'index': 2,\n",
       "  'shape': array([10], dtype=int32),\n",
       "  'shape_signature': array([10], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (3.7200734368525445e-05, 0),\n",
       "  'quantization_parameters': {'scales': array([3.7200734e-05], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/dense_1/MatMul',\n",
       "  'index': 3,\n",
       "  'shape': array([10, 84], dtype=int32),\n",
       "  'shape_signature': array([10, 84], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.004761693999171257, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00476169], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/dense/BiasAdd/ReadVariableOp',\n",
       "  'index': 4,\n",
       "  'shape': array([84], dtype=int32),\n",
       "  'shape_signature': array([84], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (3.997467501903884e-05, 0),\n",
       "  'quantization_parameters': {'scales': array([3.9974675e-05], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/dense/MatMul',\n",
       "  'index': 5,\n",
       "  'shape': array([ 84, 120], dtype=int32),\n",
       "  'shape_signature': array([ 84, 120], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.005116758402436972, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00511676], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_2/conv2d_2/BiasAdd/ReadVariableOp',\n",
       "  'index': 6,\n",
       "  'shape': array([120], dtype=int32),\n",
       "  'shape_signature': array([120], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([6.1193323e-06, 5.0782319e-06, 3.7486463e-06, 6.3105263e-06,\n",
       "          6.4221513e-06, 5.2682340e-06, 4.0402992e-06, 5.9686445e-06,\n",
       "          5.8788019e-06, 6.0122711e-06, 5.2137220e-06, 4.8965317e-06,\n",
       "          4.9223004e-06, 4.3616656e-06, 5.4446668e-06, 6.1272685e-06,\n",
       "          5.2354903e-06, 4.5301626e-06, 4.8573802e-06, 6.5909358e-06,\n",
       "          5.6131530e-06, 5.3394060e-06, 5.6932031e-06, 6.3511202e-06,\n",
       "          5.3477147e-06, 5.4524003e-06, 4.6216674e-06, 4.0496784e-06,\n",
       "          5.3106437e-06, 5.8246410e-06, 4.1041208e-06, 5.2190353e-06,\n",
       "          5.2908449e-06, 7.5042444e-06, 6.5125387e-06, 5.9275144e-06,\n",
       "          5.0168046e-06, 4.0997670e-06, 5.3656950e-06, 7.6334654e-06,\n",
       "          6.4096912e-06, 4.3271057e-06, 5.3077438e-06, 5.2479104e-06,\n",
       "          5.5823270e-06, 4.8054262e-06, 5.4773100e-06, 4.9682817e-06,\n",
       "          4.9494661e-06, 5.6729059e-06, 5.2580881e-06, 5.9475633e-06,\n",
       "          5.6633448e-06, 5.3192939e-06, 5.4320176e-06, 5.6801441e-06,\n",
       "          6.2225017e-06, 5.7213551e-06, 6.2874997e-06, 5.9016925e-06,\n",
       "          6.9618727e-06, 5.7705060e-06, 4.8328111e-06, 6.4030696e-06,\n",
       "          5.1029419e-06, 6.6521193e-06, 5.7068664e-06, 6.6589109e-06,\n",
       "          5.3552781e-06, 5.4329016e-06, 4.7170406e-06, 6.7353217e-06,\n",
       "          5.4292204e-06, 6.0572147e-06, 6.2184276e-06, 6.2859581e-06,\n",
       "          7.1317863e-06, 6.6954308e-06, 6.8882186e-06, 6.9123575e-06,\n",
       "          5.3119552e-06, 5.6606204e-06, 6.1346359e-06, 6.2975655e-06,\n",
       "          5.5139512e-06, 6.2033964e-06, 5.5940009e-06, 4.4541457e-06,\n",
       "          5.4854968e-06, 5.4121456e-06, 6.1925612e-06, 4.8076167e-06,\n",
       "          6.1539854e-06, 5.4048601e-06, 5.9080335e-06, 6.0350526e-06,\n",
       "          5.5489745e-06, 4.4773142e-06, 5.5679679e-06, 5.6406670e-06,\n",
       "          8.3921650e-06, 6.8764948e-06, 5.0229764e-06, 7.2383218e-06,\n",
       "          5.9409786e-06, 5.9178178e-06, 4.5977445e-06, 5.4469419e-06,\n",
       "          6.1783308e-06, 6.5695949e-06, 5.3503472e-06, 6.0240618e-06,\n",
       "          6.1952956e-06, 4.5848983e-06, 5.6781109e-06, 5.4569036e-06,\n",
       "          5.9867843e-06, 6.8319050e-06, 6.3253533e-06, 5.9187646e-06],\n",
       "         dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_2/conv2d_2/Conv2D',\n",
       "  'index': 7,\n",
       "  'shape': array([120,   5,   5,  16], dtype=int32),\n",
       "  'shape_signature': array([120,   5,   5,  16], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00156655, 0.00130003, 0.00095965, 0.00161549, 0.00164407,\n",
       "          0.00134867, 0.00103432, 0.00152797, 0.00150497, 0.00153914,\n",
       "          0.00133471, 0.00125351, 0.00126011, 0.00111659, 0.00139383,\n",
       "          0.00156858, 0.00134029, 0.00115972, 0.00124349, 0.00168728,\n",
       "          0.00143697, 0.00136689, 0.00145746, 0.00162589, 0.00136901,\n",
       "          0.00139581, 0.00118315, 0.00103672, 0.00135952, 0.00149111,\n",
       "          0.00105065, 0.00133607, 0.00135446, 0.00192109, 0.00166721,\n",
       "          0.00151744, 0.0012843 , 0.00104954, 0.00137362, 0.00195417,\n",
       "          0.00164088, 0.00110774, 0.00135878, 0.00134347, 0.00142908,\n",
       "          0.00123019, 0.00140219, 0.00127188, 0.00126706, 0.00145226,\n",
       "          0.00134607, 0.00152258, 0.00144982, 0.00136174, 0.0013906 ,\n",
       "          0.00145412, 0.00159296, 0.00146467, 0.0016096 , 0.00151083,\n",
       "          0.00178224, 0.00147725, 0.0012372 , 0.00163919, 0.00130635,\n",
       "          0.00170294, 0.00146096, 0.00170468, 0.00137095, 0.00139082,\n",
       "          0.00120756, 0.00172424, 0.00138988, 0.00155065, 0.00159192,\n",
       "          0.00160921, 0.00182574, 0.00171403, 0.00176338, 0.00176956,\n",
       "          0.00135986, 0.00144912, 0.00157047, 0.00161218, 0.00141157,\n",
       "          0.00158807, 0.00143206, 0.00114026, 0.00140429, 0.00138551,\n",
       "          0.0015853 , 0.00123075, 0.00157542, 0.00138364, 0.00151246,\n",
       "          0.00154497, 0.00142054, 0.00114619, 0.0014254 , 0.00144401,\n",
       "          0.00214839, 0.00176038, 0.00128588, 0.00185301, 0.00152089,\n",
       "          0.00151496, 0.00117702, 0.00139442, 0.00158165, 0.00168182,\n",
       "          0.00136969, 0.00154216, 0.001586  , 0.00117373, 0.0014536 ,\n",
       "          0.00139697, 0.00153262, 0.00174897, 0.00161929, 0.0015152 ],\n",
       "         dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_1/conv2d_1/BiasAdd/ReadVariableOp',\n",
       "  'index': 8,\n",
       "  'shape': array([16], dtype=int32),\n",
       "  'shape_signature': array([16], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([1.4442178e-05, 1.2244815e-05, 5.8405981e-06, 5.5668870e-06,\n",
       "          1.1260083e-05, 1.2118382e-05, 5.4796042e-06, 1.2269288e-05,\n",
       "          6.2812328e-06, 1.5399637e-05, 6.0751613e-06, 5.7889688e-06,\n",
       "          6.1792530e-06, 5.9300719e-06, 5.3752792e-06, 1.1114510e-05],\n",
       "         dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_1/conv2d_1/Conv2D',\n",
       "  'index': 9,\n",
       "  'shape': array([16,  5,  5,  6], dtype=int32),\n",
       "  'shape_signature': array([16,  5,  5,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0036972 , 0.00313467, 0.00149519, 0.00142512, 0.00288258,\n",
       "          0.00310231, 0.00140278, 0.00314094, 0.001608  , 0.00394231,\n",
       "          0.00155524, 0.00148198, 0.00158189, 0.0015181 , 0.00137607,\n",
       "          0.00284531], dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential/conv2d/BiasAdd/ReadVariableOp',\n",
       "  'index': 10,\n",
       "  'shape': array([6], dtype=int32),\n",
       "  'shape_signature': array([6], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([2.3021070e-05, 2.8138382e-05, 3.0988525e-05, 2.4449737e-05,\n",
       "          2.8222974e-05, 3.3232605e-05], dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential/conv2d/Conv2D',\n",
       "  'index': 11,\n",
       "  'shape': array([6, 5, 5, 1], dtype=int32),\n",
       "  'shape_signature': array([6, 5, 5, 1], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00587037, 0.00717529, 0.00790207, 0.00623468, 0.00719686,\n",
       "          0.00847431], dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential/conv2d/BiasAdd;lenet/sequential/conv2d/Conv2D;lenet/sequential/conv2d/BiasAdd/ReadVariableOp',\n",
       "  'index': 12,\n",
       "  'shape': array([ 1, 28, 28,  6], dtype=int32),\n",
       "  'shape_signature': array([-1, 28, 28,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.05533444136381149, -12),\n",
       "  'quantization_parameters': {'scales': array([0.05533444], dtype=float32),\n",
       "   'zero_points': array([-12], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential/conv2d/Tanh',\n",
       "  'index': 13,\n",
       "  'shape': array([ 1, 28, 28,  6], dtype=int32),\n",
       "  'shape_signature': array([-1, 28, 28,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential/average_pooling2d/AvgPool',\n",
       "  'index': 14,\n",
       "  'shape': array([ 1, 14, 14,  6], dtype=int32),\n",
       "  'shape_signature': array([-1, 14, 14,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential/activation/Sigmoid',\n",
       "  'index': 15,\n",
       "  'shape': array([ 1, 14, 14,  6], dtype=int32),\n",
       "  'shape_signature': array([-1, 14, 14,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.00390625, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_1/conv2d_1/BiasAdd;lenet/sequential_1/conv2d_1/Conv2D;lenet/sequential_1/conv2d_1/BiasAdd/ReadVariableOp',\n",
       "  'index': 16,\n",
       "  'shape': array([ 1, 10, 10, 16], dtype=int32),\n",
       "  'shape_signature': array([-1, 10, 10, 16], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.04205252602696419, 28),\n",
       "  'quantization_parameters': {'scales': array([0.04205253], dtype=float32),\n",
       "   'zero_points': array([28], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_1/conv2d_1/Tanh',\n",
       "  'index': 17,\n",
       "  'shape': array([ 1, 10, 10, 16], dtype=int32),\n",
       "  'shape_signature': array([-1, 10, 10, 16], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_1/average_pooling2d_1/AvgPool',\n",
       "  'index': 18,\n",
       "  'shape': array([ 1,  5,  5, 16], dtype=int32),\n",
       "  'shape_signature': array([-1,  5,  5, 16], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_1/activation_1/Sigmoid',\n",
       "  'index': 19,\n",
       "  'shape': array([ 1,  5,  5, 16], dtype=int32),\n",
       "  'shape_signature': array([-1,  5,  5, 16], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.00390625, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_2/conv2d_2/BiasAdd;lenet/sequential_2/conv2d_2/Conv2D;lenet/sequential_2/conv2d_2/BiasAdd/ReadVariableOp',\n",
       "  'index': 20,\n",
       "  'shape': array([  1,   1,   1, 120], dtype=int32),\n",
       "  'shape_signature': array([ -1,   1,   1, 120], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.010792302899062634, 10),\n",
       "  'quantization_parameters': {'scales': array([0.0107923], dtype=float32),\n",
       "   'zero_points': array([10], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_2/conv2d_2/Tanh',\n",
       "  'index': 21,\n",
       "  'shape': array([  1,   1,   1, 120], dtype=int32),\n",
       "  'shape_signature': array([ -1,   1,   1, 120], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/sequential_2/flatten/Reshape',\n",
       "  'index': 22,\n",
       "  'shape': array([  1, 120], dtype=int32),\n",
       "  'shape_signature': array([ -1, 120], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/dense/MatMul;lenet/dense/BiasAdd',\n",
       "  'index': 23,\n",
       "  'shape': array([ 1, 84], dtype=int32),\n",
       "  'shape_signature': array([-1, 84], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.06151371821761131, 7),\n",
       "  'quantization_parameters': {'scales': array([0.06151372], dtype=float32),\n",
       "   'zero_points': array([7], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/dense/Tanh',\n",
       "  'index': 24,\n",
       "  'shape': array([ 1, 84], dtype=int32),\n",
       "  'shape_signature': array([-1, 84], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet/dense_1/MatMul;lenet/dense_1/BiasAdd',\n",
       "  'index': 25,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.09972357749938965, 5),\n",
       "  'quantization_parameters': {'scales': array([0.09972358], dtype=float32),\n",
       "   'zero_points': array([5], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'StatefulPartitionedCall:0',\n",
       "  'index': 26,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.00390625, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"lenet5_mnist.tflite\", experimental_preserve_all_tensors=True)\n",
    "interpreter_edited = tf.lite.Interpreter(model_path=\"lenet5_edited.tflite\")\n",
    "\n",
    "tf.lite.experimental.Analyzer.analyze(model_path=\"lenet5_edited.tflite\")\n",
    "tf.lite.experimental.Analyzer.analyze(model_path=\"lenet5_mnist.tflite\")\n",
    "\n",
    "interpreter.get_tensor_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXWtHAfJJrsq",
    "outputId": "6844da5e-c10f-4413-b1ab-7ae76c958556"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_args_0:0',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 32, 32,  1], dtype=int32),\n",
       "  'shape_signature': array([-1, 32, 32,  1], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.003921568859368563, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'serving_default_args_1:0',\n",
       "  'index': 1,\n",
       "  'shape': array([  1, 120], dtype=int32),\n",
       "  'shape_signature': array([ -1, 120], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_63/flatten_20/Const',\n",
       "  'index': 2,\n",
       "  'shape': array([2], dtype=int32),\n",
       "  'shape_signature': array([2], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_63/conv2d_63/BiasAdd/ReadVariableOp',\n",
       "  'index': 3,\n",
       "  'shape': array([120], dtype=int32),\n",
       "  'shape_signature': array([120], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([6.1193323e-06, 5.0782319e-06, 3.7486463e-06, 6.3105263e-06,\n",
       "          6.4221513e-06, 5.2682340e-06, 4.0402992e-06, 5.9686445e-06,\n",
       "          5.8788019e-06, 6.0122711e-06, 5.2137220e-06, 4.8965317e-06,\n",
       "          4.9223004e-06, 4.3616656e-06, 5.4446668e-06, 6.1272685e-06,\n",
       "          5.2354903e-06, 4.5301626e-06, 4.8573802e-06, 6.5909358e-06,\n",
       "          5.6131530e-06, 5.3394060e-06, 5.6932031e-06, 6.3511202e-06,\n",
       "          5.3477147e-06, 5.4524003e-06, 4.6216674e-06, 4.0496784e-06,\n",
       "          5.3106437e-06, 5.8246410e-06, 4.1041208e-06, 5.2190353e-06,\n",
       "          5.2908449e-06, 7.5042444e-06, 6.5125387e-06, 5.9275144e-06,\n",
       "          5.0168046e-06, 4.0997670e-06, 5.3656950e-06, 7.6334654e-06,\n",
       "          6.4096912e-06, 4.3271057e-06, 5.3077438e-06, 5.2479104e-06,\n",
       "          5.5823270e-06, 4.8054262e-06, 5.4773100e-06, 4.9682817e-06,\n",
       "          4.9494661e-06, 5.6729059e-06, 5.2580881e-06, 5.9475633e-06,\n",
       "          5.6633448e-06, 5.3192939e-06, 5.4320176e-06, 5.6801441e-06,\n",
       "          6.2225017e-06, 5.7213551e-06, 6.2874997e-06, 5.9016925e-06,\n",
       "          6.9618727e-06, 5.7705060e-06, 4.8328111e-06, 6.4030696e-06,\n",
       "          5.1029419e-06, 6.6521193e-06, 5.7068664e-06, 6.6589109e-06,\n",
       "          5.3552781e-06, 5.4329016e-06, 4.7170406e-06, 6.7353217e-06,\n",
       "          5.4292204e-06, 6.0572147e-06, 6.2184276e-06, 6.2859581e-06,\n",
       "          7.1317863e-06, 6.6954308e-06, 6.8882186e-06, 6.9123575e-06,\n",
       "          5.3119552e-06, 5.6606204e-06, 6.1346359e-06, 6.2975655e-06,\n",
       "          5.5139512e-06, 6.2033964e-06, 5.5940009e-06, 4.4541457e-06,\n",
       "          5.4854968e-06, 5.4121456e-06, 6.1925612e-06, 4.8076167e-06,\n",
       "          6.1539854e-06, 5.4048601e-06, 5.9080335e-06, 6.0350526e-06,\n",
       "          5.5489745e-06, 4.4773142e-06, 5.5679679e-06, 5.6406670e-06,\n",
       "          8.3921650e-06, 6.8764948e-06, 5.0229764e-06, 7.2383218e-06,\n",
       "          5.9409786e-06, 5.9178178e-06, 4.5977445e-06, 5.4469419e-06,\n",
       "          6.1783308e-06, 6.5695949e-06, 5.3503472e-06, 6.0240618e-06,\n",
       "          6.1952956e-06, 4.5848983e-06, 5.6781109e-06, 5.4569036e-06,\n",
       "          5.9867843e-06, 6.8319050e-06, 6.3253533e-06, 5.9187646e-06],\n",
       "         dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_63/conv2d_63/Conv2D',\n",
       "  'index': 4,\n",
       "  'shape': array([120,   5,   5,  16], dtype=int32),\n",
       "  'shape_signature': array([120,   5,   5,  16], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00156655, 0.00130003, 0.00095965, 0.00161549, 0.00164407,\n",
       "          0.00134867, 0.00103432, 0.00152797, 0.00150497, 0.00153914,\n",
       "          0.00133471, 0.00125351, 0.00126011, 0.00111659, 0.00139383,\n",
       "          0.00156858, 0.00134029, 0.00115972, 0.00124349, 0.00168728,\n",
       "          0.00143697, 0.00136689, 0.00145746, 0.00162589, 0.00136901,\n",
       "          0.00139581, 0.00118315, 0.00103672, 0.00135952, 0.00149111,\n",
       "          0.00105065, 0.00133607, 0.00135446, 0.00192109, 0.00166721,\n",
       "          0.00151744, 0.0012843 , 0.00104954, 0.00137362, 0.00195417,\n",
       "          0.00164088, 0.00110774, 0.00135878, 0.00134347, 0.00142908,\n",
       "          0.00123019, 0.00140219, 0.00127188, 0.00126706, 0.00145226,\n",
       "          0.00134607, 0.00152258, 0.00144982, 0.00136174, 0.0013906 ,\n",
       "          0.00145412, 0.00159296, 0.00146467, 0.0016096 , 0.00151083,\n",
       "          0.00178224, 0.00147725, 0.0012372 , 0.00163919, 0.00130635,\n",
       "          0.00170294, 0.00146096, 0.00170468, 0.00137095, 0.00139082,\n",
       "          0.00120756, 0.00172424, 0.00138988, 0.00155065, 0.00159192,\n",
       "          0.00160921, 0.00182574, 0.00171403, 0.00176338, 0.00176956,\n",
       "          0.00135986, 0.00144912, 0.00157047, 0.00161218, 0.00141157,\n",
       "          0.00158807, 0.00143206, 0.00114026, 0.00140429, 0.00138551,\n",
       "          0.0015853 , 0.00123075, 0.00157542, 0.00138364, 0.00151246,\n",
       "          0.00154497, 0.00142054, 0.00114619, 0.0014254 , 0.00144401,\n",
       "          0.00214839, 0.00176038, 0.00128588, 0.00185301, 0.00152089,\n",
       "          0.00151496, 0.00117702, 0.00139442, 0.00158165, 0.00168182,\n",
       "          0.00136969, 0.00154216, 0.001586  , 0.00117373, 0.0014536 ,\n",
       "          0.00139697, 0.00153262, 0.00174897, 0.00161929, 0.0015152 ],\n",
       "         dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_62/conv2d_62/BiasAdd/ReadVariableOp',\n",
       "  'index': 5,\n",
       "  'shape': array([16], dtype=int32),\n",
       "  'shape_signature': array([16], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([1.4442178e-05, 1.2244815e-05, 5.8405981e-06, 5.5668870e-06,\n",
       "          1.1260083e-05, 1.2118382e-05, 5.4796042e-06, 1.2269288e-05,\n",
       "          6.2812328e-06, 1.5399637e-05, 6.0751613e-06, 5.7889688e-06,\n",
       "          6.1792530e-06, 5.9300719e-06, 5.3752792e-06, 1.1114510e-05],\n",
       "         dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_62/conv2d_62/Conv2D',\n",
       "  'index': 6,\n",
       "  'shape': array([16,  5,  5,  6], dtype=int32),\n",
       "  'shape_signature': array([16,  5,  5,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0036972 , 0.00313467, 0.00149519, 0.00142512, 0.00288258,\n",
       "          0.00310231, 0.00140278, 0.00314094, 0.001608  , 0.00394231,\n",
       "          0.00155524, 0.00148198, 0.00158189, 0.0015181 , 0.00137607,\n",
       "          0.00284531], dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_61/conv2d_61/BiasAdd/ReadVariableOp',\n",
       "  'index': 7,\n",
       "  'shape': array([6], dtype=int32),\n",
       "  'shape_signature': array([6], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([2.3021070e-05, 2.8138382e-05, 3.0988525e-05, 2.4449737e-05,\n",
       "          2.8222974e-05, 3.3232605e-05], dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_61/conv2d_61/Conv2D',\n",
       "  'index': 8,\n",
       "  'shape': array([6, 5, 5, 1], dtype=int32),\n",
       "  'shape_signature': array([6, 5, 5, 1], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00587037, 0.00717529, 0.00790207, 0.00623468, 0.00719686,\n",
       "          0.00847431], dtype=float32),\n",
       "   'zero_points': array([0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_41/BiasAdd/ReadVariableOp',\n",
       "  'index': 9,\n",
       "  'shape': array([10], dtype=int32),\n",
       "  'shape_signature': array([10], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (3.7200734368525445e-05, 0),\n",
       "  'quantization_parameters': {'scales': array([3.7200734e-05], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_41/BiasAdd/ReadVariableOp1',\n",
       "  'index': 10,\n",
       "  'shape': array([10], dtype=int32),\n",
       "  'shape_signature': array([10], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (3.7200734368525445e-05, 0),\n",
       "  'quantization_parameters': {'scales': array([3.7200734e-05], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_41/MatMul_1',\n",
       "  'index': 11,\n",
       "  'shape': array([10, 84], dtype=int32),\n",
       "  'shape_signature': array([10, 84], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.004761693999171257, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00476169], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_40/BiasAdd/ReadVariableOp',\n",
       "  'index': 12,\n",
       "  'shape': array([84], dtype=int32),\n",
       "  'shape_signature': array([84], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (3.997467501903884e-05, 0),\n",
       "  'quantization_parameters': {'scales': array([3.9974675e-05], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_40/BiasAdd/ReadVariableOp1',\n",
       "  'index': 13,\n",
       "  'shape': array([84], dtype=int32),\n",
       "  'shape_signature': array([84], dtype=int32),\n",
       "  'dtype': numpy.int32,\n",
       "  'quantization': (3.997467501903884e-05, 0),\n",
       "  'quantization_parameters': {'scales': array([3.9974675e-05], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_40/MatMul_1',\n",
       "  'index': 14,\n",
       "  'shape': array([ 84, 120], dtype=int32),\n",
       "  'shape_signature': array([ 84, 120], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.005116758402436972, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00511676], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_40/MatMul;lenet_21/dense_40/BiasAdd',\n",
       "  'index': 15,\n",
       "  'shape': array([ 1, 84], dtype=int32),\n",
       "  'shape_signature': array([-1, 84], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.061513714492321014, 7),\n",
       "  'quantization_parameters': {'scales': array([0.06151371], dtype=float32),\n",
       "   'zero_points': array([7], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_40/Tanh',\n",
       "  'index': 16,\n",
       "  'shape': array([ 1, 84], dtype=int32),\n",
       "  'shape_signature': array([-1, 84], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_41/MatMul;lenet_21/dense_41/BiasAdd',\n",
       "  'index': 17,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.10003606230020523, 5),\n",
       "  'quantization_parameters': {'scales': array([0.10003606], dtype=float32),\n",
       "   'zero_points': array([5], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'StatefulPartitionedCall:1',\n",
       "  'index': 18,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.00390625, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_61/conv2d_61/BiasAdd;lenet_21/sequential_61/conv2d_61/Conv2D;lenet_21/sequential_61/conv2d_61/BiasAdd/ReadVariableOp',\n",
       "  'index': 19,\n",
       "  'shape': array([ 1, 28, 28,  6], dtype=int32),\n",
       "  'shape_signature': array([-1, 28, 28,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.05561614781618118, -12),\n",
       "  'quantization_parameters': {'scales': array([0.05561615], dtype=float32),\n",
       "   'zero_points': array([-12], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_61/conv2d_61/Tanh',\n",
       "  'index': 20,\n",
       "  'shape': array([ 1, 28, 28,  6], dtype=int32),\n",
       "  'shape_signature': array([-1, 28, 28,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_61/average_pooling2d_41/AvgPool',\n",
       "  'index': 21,\n",
       "  'shape': array([ 1, 14, 14,  6], dtype=int32),\n",
       "  'shape_signature': array([-1, 14, 14,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_61/activation_40/Sigmoid',\n",
       "  'index': 22,\n",
       "  'shape': array([ 1, 14, 14,  6], dtype=int32),\n",
       "  'shape_signature': array([-1, 14, 14,  6], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.00390625, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_62/conv2d_62/BiasAdd;lenet_21/sequential_62/conv2d_62/Conv2D;lenet_21/sequential_62/conv2d_62/BiasAdd/ReadVariableOp',\n",
       "  'index': 23,\n",
       "  'shape': array([ 1, 10, 10, 16], dtype=int32),\n",
       "  'shape_signature': array([-1, 10, 10, 16], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.04252227023243904, 28),\n",
       "  'quantization_parameters': {'scales': array([0.04252227], dtype=float32),\n",
       "   'zero_points': array([28], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_62/conv2d_62/Tanh',\n",
       "  'index': 24,\n",
       "  'shape': array([ 1, 10, 10, 16], dtype=int32),\n",
       "  'shape_signature': array([-1, 10, 10, 16], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_62/average_pooling2d_42/AvgPool',\n",
       "  'index': 25,\n",
       "  'shape': array([ 1,  5,  5, 16], dtype=int32),\n",
       "  'shape_signature': array([-1,  5,  5, 16], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_62/activation_41/Sigmoid',\n",
       "  'index': 26,\n",
       "  'shape': array([ 1,  5,  5, 16], dtype=int32),\n",
       "  'shape_signature': array([-1,  5,  5, 16], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.00390625, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_63/conv2d_63/BiasAdd;lenet_21/sequential_63/conv2d_63/Conv2D;lenet_21/sequential_63/conv2d_63/BiasAdd/ReadVariableOp',\n",
       "  'index': 27,\n",
       "  'shape': array([  1,   1,   1, 120], dtype=int32),\n",
       "  'shape_signature': array([ -1,   1,   1, 120], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.011838337406516075, 18),\n",
       "  'quantization_parameters': {'scales': array([0.01183834], dtype=float32),\n",
       "   'zero_points': array([18], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_63/conv2d_63/Tanh',\n",
       "  'index': 28,\n",
       "  'shape': array([  1,   1,   1, 120], dtype=int32),\n",
       "  'shape_signature': array([ -1,   1,   1, 120], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/sequential_63/flatten_20/Reshape',\n",
       "  'index': 29,\n",
       "  'shape': array([  1, 120], dtype=int32),\n",
       "  'shape_signature': array([ -1, 120], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_40/MatMul_1;lenet_21/dense_40/BiasAdd_1',\n",
       "  'index': 30,\n",
       "  'shape': array([ 1, 84], dtype=int32),\n",
       "  'shape_signature': array([-1, 84], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.06151371821761131, 7),\n",
       "  'quantization_parameters': {'scales': array([0.06151372], dtype=float32),\n",
       "   'zero_points': array([7], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_40/Tanh_1',\n",
       "  'index': 31,\n",
       "  'shape': array([ 1, 84], dtype=int32),\n",
       "  'shape_signature': array([-1, 84], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0078125, 0),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'lenet_21/dense_41/MatMul_1;lenet_21/dense_41/BiasAdd_1',\n",
       "  'index': 32,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.09972357749938965, 5),\n",
       "  'quantization_parameters': {'scales': array([0.09972358], dtype=float32),\n",
       "   'zero_points': array([5], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'StatefulPartitionedCall:0',\n",
       "  'index': 33,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.00390625, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([-128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter_edited.get_tensor_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7Y0XFL9ZSP8"
   },
   "source": [
    "###Input Tensor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rfqvH5muzCf6",
    "outputId": "6ace6fdd-784c-4e22-e192-983bbd46e3e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input scale: 0.003921568859368563\n",
      "Zero point: -128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'out': array([[-118, -128,  -67,  -36, -128, -119, -128, -128,  -45, -127]],\n",
       "       dtype=int8),\n",
       " 'out_dense': array([[-105, -101,  -94, -105, -108,  -94,  -98, -114,  -94, -110]],\n",
       "       dtype=int8)}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details = interpreter.get_input_details()[0]\n",
    "input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "print(f\"Input scale: {input_scale}\")\n",
    "print(f\"Zero point: {input_zero_point}\")\n",
    "\n",
    "sig = interpreter.get_signature_runner()\n",
    "interpreter.get_signature_list()\n",
    "sig(args_0=tf.ones((1,32,32,1), tf.int8), args_1=tf.ones((1,120), tf.int8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyEWUuLMlg6D"
   },
   "source": [
    "##Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ui1-WHOCATAJ"
   },
   "source": [
    "Quantize the validation set and cast to int8, then calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKXSAmEjuefJ",
    "outputId": "b8df3c40-b420-4cf6-ba21-24de6b06e8b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy on validation set: 95.80\n"
     ]
    }
   ],
   "source": [
    "quantized_val = tf.cast(x_val / input_scale + input_zero_point, tf.int8)\n",
    "outputs = sig(input_1=quantized_val)['output_1']\n",
    "quantized_pred = tf.argmax(outputs, axis=1)\n",
    "print(\"Quantized model accuracy on validation set: {:.02f}\".format((tf.math.count_nonzero(quantized_pred == y_val) / 2000) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zL5LZYE7ljqh"
   },
   "source": [
    "##Save tensor as an array of bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-gVayfyCfBF"
   },
   "source": [
    "Save a Tensor in binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_zO3vXRCI6Y"
   },
   "outputs": [],
   "source": [
    "image = quantized_val[0]\n",
    "with open('image.bin', 'wb') as f:\n",
    "  f.write(bytearray(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNhqMZoxsN3S"
   },
   "source": [
    "Generate the array of char from the .bin file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8i_Tqxhsrnwu",
    "outputId": "f14f4603-f8ab-4410-9784-cca3f83f4628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header file 'image_input.h' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "command = [\"xxd\", \"-i\", 'image.bin']\n",
    "input_file_name = 'image.bin'\n",
    "output_file_name = 'image_input.h'\n",
    "\n",
    "with open(output_file_name, \"w\") as outfile:\n",
    "    result = subprocess.run(command, stdout=outfile, text=True)\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Header file '{output_file_name}' generated successfully.\")\n",
    "    else:\n",
    "        print(\"Error generating header file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaQUTKqFU2uU"
   },
   "source": [
    "# Get Golden Output with a given input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HggcOFCmEPOa",
    "outputId": "7e95a6e0-4ab2-4629-862f-c3fe4b3aebcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[0.09999989, 0.10000001, 0.10000007, 0.10000016, 0.09999985,\n",
      "        0.09999989, 0.09999941, 0.10000043, 0.10000008, 0.10000022]],\n",
      "      dtype=float32)>, 'out_dense': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[0.09420101, 0.09051015, 0.11445776, 0.10434542, 0.09869833,\n",
      "        0.11275383, 0.08072697, 0.09569866, 0.10742386, 0.10118403]],\n",
      "      dtype=float32)>}\n",
      "[[-105 -102 -102 -102 -102 -102 -102 -102 -102 -102]]\n",
      "[[-102 -102 -104 -102 -102 -102 -102 -104 -102 -102]]\n",
      "{'out': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[7.7119657e-05, 8.4893960e-01, 1.2776609e-02, 2.4631536e-03,\n",
      "        1.7322188e-02, 1.3093193e-03, 2.1620376e-02, 9.2544287e-02,\n",
      "        2.5923742e-04, 2.6881429e-03]], dtype=float32)>, 'out_dense': <tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
      "array([[0.09420101, 0.09051015, 0.11445776, 0.10434542, 0.09869833,\n",
      "        0.11275383, 0.08072697, 0.09569866, 0.10742386, 0.10118403]],\n",
      "      dtype=float32)>}\n",
      ".npy file content as f32: [0.08984375 0.1015625  0.1015625  0.1015625  0.1015625  0.1015625\n",
      " 0.1015625  0.1015625  0.1015625  0.1015625 ]\n",
      "Predicted percentages:\n",
      "Class 0: 8.98%\n",
      "Class 1: 10.16%\n",
      "Class 2: 10.16%\n",
      "Class 3: 10.16%\n",
      "Class 4: 10.16%\n",
      "Class 5: 10.16%\n",
      "Class 6: 10.16%\n",
      "Class 7: 10.16%\n",
      "Class 8: 10.16%\n",
      "Class 9: 10.16%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference output data: [[0.08984375 0.1015625  0.1015625  0.1015625  0.1015625  0.1015625\n",
      "  0.1015625  0.1015625  0.1015625  0.1015625 ]]\n",
      ".npy file content: [[0.08984375 0.1015625  0.1015625  0.1015625  0.1015625  0.1015625\n",
      "  0.1015625  0.1015625  0.1015625  0.1015625 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# ----------- Quantized model inference -------------\n",
    "interpreter = tf.lite.Interpreter(model_path=\"lenet5_edited.tflite\", experimental_preserve_all_tensors=True)\n",
    "interpreter_edited = tf.lite.Interpreter(model_path=\"lenet5_edited.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "in_scale, in_zero_point = interpreter.get_input_details()[0][\"quantization\"]\n",
    "out_scale, out_zero_point = interpreter.get_output_details()[0][\"quantization\"]\n",
    "# ---------- END Quantized model inference -----------\n",
    "# --------- NOT Quantized model inference ------------\n",
    "\"\"\"\n",
    "model = tf.keras.models.load_model(\"lenet5.keras\", custom_objects={\"Lenet\": Lenet})\n",
    "in_zero_point = 128\n",
    "in_scale = 0.003921568859368563\n",
    "out_scale = 0.00390625\n",
    "out_zero_point = 128\n",
    "\"\"\"\n",
    "# -------- END NOT Quantized model inference ---------\n",
    "\n",
    "def read_png_input_image():\n",
    "  input_img = np.load(\"input_image_float32.npy\")\n",
    "  input_img = np.expand_dims(input_img, axis=0)\n",
    "\n",
    "  print(model(input_img))\n",
    "\n",
    "def check_patterns(image):\n",
    "  coords = [(15,9),(15,10),(15,11),(15,12),(15,13),(15,14),(15,15),(15,16),(15,17),(15,18),(14,10),(13,11),(12,12),(11,18),(12,18),(13,18),(14,18),(16,18),(17,18),(18,18),(19,18)]\n",
    "  hex_values = ['49', 'e4', '0a', 'd8', '7c', 'f7', '71', 'ae', '7e', 'a6', 'c8', '01', '87', 'f6', '5d', 'f6', '79', 'a6', '3a', 'f2', '7b']\n",
    "  # Convert the hex values to a numerical representation (float32)\n",
    "  # Cast the uint8 values to float32\n",
    "  #float32_values = tf.cast(int8_values, tf.float32)\n",
    "\n",
    "  # Assign the float32 values to the specified coordinates\n",
    "  for i, coord in enumerate(coords):\n",
    "    image[0, coord[0], coord[1], 0] = np.int8(int(hex_values[i], 16))\n",
    "\n",
    "def read_hex_input_image():\n",
    "  with open('./input_image_class_visualization.hex', 'r') as f:\n",
    "      values_str = f.read()\n",
    "\n",
    "  # Convert the hex string to bytes\n",
    "  image_bytes = bytes.fromhex(values_str)\n",
    "\n",
    "  # Convert the bytes to a numpy array\n",
    "  int_values = np.frombuffer(image_bytes, dtype=np.int8)\n",
    "\n",
    "  # Reshape the integer values to the correct shape for the model\n",
    "  input_data = np.load(\"input_image_int8.npy\").reshape((1, 32, 32, 1))\n",
    "  input_data_float = np.load(\"input_image_int8.npy\").reshape((1, 32, 32, 1))\n",
    "  #print(out_model)\n",
    "\n",
    "  # ----------- Quantized model inference -------------\n",
    "\n",
    "  # Get input and output tensors\n",
    "  input_details = interpreter.get_input_details()\n",
    "  output_details = interpreter.get_output_details()\n",
    "  # Get the output tensor\n",
    "  interpreter.allocate_tensors()\n",
    "  interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "  interpreter.invoke()\n",
    "  tanh_conv3 = interpreter.get_tensor(29)\n",
    "  interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "  interpreter.set_tensor(input_details[1]['index'], tanh_conv3)\n",
    "  interpreter.invoke()\n",
    "  output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "  output_dense = interpreter.get_tensor(output_details[1]['index'])\n",
    "  print(output_data)\n",
    "  print(output_dense)\n",
    "  print(model(input_data_float))\n",
    "  output_data = out_scale * (output_data + out_zero_point)\n",
    "  for out in output_data:\n",
    "    print(f\".npy file content as f32: {out}\")\n",
    "\n",
    "  predicted_percentages = output_data * 100\n",
    "  print(\"Predicted percentages:\")\n",
    "  for i, percentage in enumerate(predicted_percentages[0]):\n",
    "    print(f\"Class {i}: {percentage:.2f}%\")\n",
    "\n",
    "  # ---------- END Quantized model inference -----------\n",
    "  # --------- NOT Quantized model inference ------------\n",
    "  \"\"\"\n",
    "  output_data = model(input_data)\n",
    "\n",
    "  output_data = out_scale * (output_data + out_zero_point)\n",
    "  for out in output_data:\n",
    "    print(f\".npy file content as f32: {out}\")\n",
    "\n",
    "  predicted_percentages = output_data * 100\n",
    "  print(\"Predicted percentages:\")\n",
    "  for i, percentage in enumerate(predicted_percentages[0]):\n",
    "    print(f\"Class {i}: {percentage:.2f}%\")\n",
    "  \"\"\"\n",
    "  # -------- END NOT Quantized model inference ---------\n",
    "\n",
    "  # Print the input image\n",
    "  plt.imshow(input_data[0], cmap='gray')\n",
    "  plt.show()\n",
    "\n",
    "  np.save('golden_output.npy', output_data)\n",
    "  print(f\"inference output data: {output_data}\")\n",
    "  out_read = np.load('golden_output.npy')\n",
    "  print(f\".npy file content: {out_read}\")\n",
    "\n",
    "\n",
    "read_png_input_image()\n",
    "read_hex_input_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmwsPjlmt_Wn"
   },
   "source": [
    "#Save Model Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxxTY6wEOvD9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"lenet5_mnist.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "in_scale, in_zero_point = interpreter.get_input_details()[0][\"quantization\"]\n",
    "out_scale, out_zero_point = interpreter.get_output_details()[0][\"quantization\"]\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "\n",
    "# conv1 tensors\n",
    "conv1_input = interpreter.get_tensor(input_details[0]['index'])\n",
    "#np.save(\"./lenet5_conv1_input_tensor\", conv1_input)\n",
    "conv1_weights = interpreter.get_tensor(11)\n",
    "#np.save(\"./lenet5_conv1_weight_tensor\", conv1_weights)\n",
    "conv1_bias = interpreter.get_tensor(10)\n",
    "#np.save(\"./lenet5_conv1_bias_tensor\", conv1_bias)\n",
    "\n",
    "# conv2 tensors\n",
    "conv2_weights = interpreter.get_tensor(9)\n",
    "#np.save(\"./lenet5_conv2_weight_tensor\", conv2_weights)\n",
    "conv2_bias = interpreter.get_tensor(8)\n",
    "#np.save(\"./lenet5_conv2_bias_tensor\", conv2_bias)\n",
    "\n",
    "# conv3 tensors\n",
    "conv3_weights = interpreter.get_tensor(7)\n",
    "np.save(\"./lenet5_conv3_weight_tensor\", conv3_weights)\n",
    "conv3_bias = interpreter.get_tensor(6)\n",
    "np.save(\"./lenet5_conv3_bias_tensor\", conv3_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L-oiE3ZwwNd"
   },
   "source": [
    "# Print Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8i9qRCtBvHCI",
    "outputId": "68f405ae-dad8-4c41-ba77-2384e2323403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-102 -102 -102 -102 -102 -104 -102 -104 -102 -102]]\n",
      "[[0.1015625 0.1015625 0.1015625 0.1015625 0.1015625 0.09375   0.1015625\n",
      "  0.09375   0.1015625 0.1015625]]\n",
      "Predicted percentages:\n",
      "Class 0: 10.16%\n",
      "Class 1: 10.16%\n",
      "Class 2: 10.16%\n",
      "Class 3: 10.16%\n",
      "Class 4: 10.16%\n",
      "Class 5: 9.38%\n",
      "Class 6: 10.16%\n",
      "Class 7: 9.38%\n",
      "Class 8: 10.16%\n",
      "Class 9: 10.16%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"lenet5_mnist.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "in_scale, in_zero_point = interpreter.get_input_details()[0][\"quantization\"]\n",
    "out_scale, out_zero_point = interpreter.get_output_details()[0][\"quantization\"]\n",
    "\n",
    "with open('./input_image_class_visualization.hex', 'r') as f:\n",
    "    values_str = f.read()\n",
    "\n",
    "# Convert the hex string to bytes\n",
    "image_bytes = bytes.fromhex(values_str)\n",
    "\n",
    "# Convert the bytes to a numpy array\n",
    "int_values = np.frombuffer(image_bytes, dtype=np.int8)\n",
    "\n",
    "# Reshape the integer values to the correct shape for the model\n",
    "input_data = int_values.reshape((1, 32, 32, 1))\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "sig = interpreter.get_signature_runner()\n",
    "\n",
    "# Get the output tensor\n",
    "output_data = sig(input_1=input_data)['output_1']\n",
    "print(output_data)\n",
    "#output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "output_data = out_scale * (output_data + out_zero_point)\n",
    "print(output_data)\n",
    "predicted_percentages = output_data * 100\n",
    "# Print the predicted percentages\n",
    "print(\"Predicted percentages:\")\n",
    "for i, percentage in enumerate(predicted_percentages[0]):\n",
    "    print(f\"Class {i}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcVTXJS5y3v5",
    "outputId": "7f801e32-a255-4261-af66-7ff1f745dd2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-102 -102 -102 -102 -102 -104 -102 -102 -104 -102]]\n",
      "Scores: \n",
      "Class 0: 10.15625%\n",
      "Class 1: 10.15625%\n",
      "Class 2: 10.15625%\n",
      "Class 3: 10.15625%\n",
      "Class 4: 10.15625%\n",
      "Class 5: 9.375%\n",
      "Class 6: 10.15625%\n",
      "Class 7: 10.15625%\n",
      "Class 8: 9.375%\n",
      "Class 9: 10.15625%\n"
     ]
    }
   ],
   "source": [
    "image_int8 = np.load(\"input_image_int8.npy\")\n",
    "interpreter = tf.lite.Interpreter(model_path=\"lenet5_mnist.tflite\")\n",
    "out_scale, out_zero_point = interpreter.get_output_details()[0][\"quantization\"]\n",
    "interpreter.allocate_tensors()\n",
    "sig = interpreter.get_signature_runner()\n",
    "output_data = sig(input_1=image_int8)['output_1']\n",
    "print(output_data)\n",
    "output_data = out_scale * (output_data + out_zero_point)\n",
    "print(\"Scores: \")\n",
    "for i, out in enumerate(output_data[0]):\n",
    "  print(f\"Class {i}: {out * 100.0}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e_CbRPFNHDP",
    "outputId": "c7c28c21-f690-4a51-83c3-54a6b05829f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 290ms/step\n",
      "Scores: \n",
      "Class 0: 9.999984502792358%\n",
      "Class 1: 9.999991208314896%\n",
      "Class 2: 10.00000610947609%\n",
      "Class 3: 10.000008344650269%\n",
      "Class 4: 9.999996423721313%\n",
      "Class 5: 10.000000149011612%\n",
      "Class 6: 9.999945759773254%\n",
      "Class 7: 10.000044107437134%\n",
      "Class 8: 10.00000536441803%\n",
      "Class 9: 10.000015795230865%\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"lenet5.keras\", custom_objects={\"Lenet\": Lenet})\n",
    "image_float32 = np.load(\"input_image_float32.npy\")\n",
    "output_data = model.predict(image_float32[None,:,:,:])\n",
    "print(\"Scores: \")\n",
    "for i, out in enumerate(output_data[0]):\n",
    "  print(f\"Class {i}: {out * 100.0}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A9ooDCHX7uD"
   },
   "source": [
    "# IICV Input Image\n",
    "generating a input image exploiting \"inverted\" class visualization and placing patterns in chosen positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sUoHUCEbYjUi",
    "outputId": "3b3c5ca8-867d-42f3-f624-fd8cfa88789c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-e34a0306fe83>:31: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 228 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  float32_values = np.array([(np.int8(int(val, 16)) - in_zero_point) * in_scale for val in hex_values], dtype=np.float32)\n",
      "<ipython-input-46-e34a0306fe83>:31: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 216 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  float32_values = np.array([(np.int8(int(val, 16)) - in_zero_point) * in_scale for val in hex_values], dtype=np.float32)\n",
      "<ipython-input-46-e34a0306fe83>:31: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 247 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  float32_values = np.array([(np.int8(int(val, 16)) - in_zero_point) * in_scale for val in hex_values], dtype=np.float32)\n",
      "<ipython-input-46-e34a0306fe83>:31: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 174 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  float32_values = np.array([(np.int8(int(val, 16)) - in_zero_point) * in_scale for val in hex_values], dtype=np.float32)\n",
      "<ipython-input-46-e34a0306fe83>:31: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 166 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  float32_values = np.array([(np.int8(int(val, 16)) - in_zero_point) * in_scale for val in hex_values], dtype=np.float32)\n",
      "<ipython-input-46-e34a0306fe83>:31: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 200 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  float32_values = np.array([(np.int8(int(val, 16)) - in_zero_point) * in_scale for val in hex_values], dtype=np.float32)\n",
      "<ipython-input-46-e34a0306fe83>:31: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 135 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  float32_values = np.array([(np.int8(int(val, 16)) - in_zero_point) * in_scale for val in hex_values], dtype=np.float32)\n",
      "<ipython-input-46-e34a0306fe83>:31: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 246 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  float32_values = np.array([(np.int8(int(val, 16)) - in_zero_point) * in_scale for val in hex_values], dtype=np.float32)\n",
      "<ipython-input-46-e34a0306fe83>:31: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 242 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  float32_values = np.array([(np.int8(int(val, 16)) - in_zero_point) * in_scale for val in hex_values], dtype=np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: [-0.56684595] Probs: [[6.83163181e-02 1.22230276e-05 1.88122801e-02 1.00768954e-02\n",
      "  1.09727924e-04 1.13614224e-01 1.32392415e-05 1.65426871e-04\n",
      "  7.78763831e-01 1.01158721e-02]]\n",
      "Step 200, Loss: [-0.8367761] Probs: [[1.4280134e-01 9.7149772e-05 1.4285566e-01 1.4285243e-01 4.1462630e-05\n",
      "  1.4286107e-01 1.7065891e-07 1.4283526e-01 1.4284080e-01 1.4281462e-01]]\n",
      "Step 400, Loss: [-0.8367768] Probs: [[1.4279959e-01 9.8239208e-05 1.4285931e-01 1.4285062e-01 4.1128165e-05\n",
      "  1.4286166e-01 1.7208671e-07 1.4283909e-01 1.4283451e-01 1.4281572e-01]]\n",
      "Step 600, Loss: [-0.83677745] Probs: [[1.4279801e-01 9.9558623e-05 1.4285885e-01 1.4285068e-01 4.0723004e-05\n",
      "  1.4286317e-01 1.7387447e-07 1.4284012e-01 1.4283434e-01 1.4281437e-01]]\n",
      "Step 800, Loss: [-0.8367784] Probs: [[1.4279655e-01 1.0112917e-04 1.4285809e-01 1.4285055e-01 4.0227631e-05\n",
      "  1.4286500e-01 1.7584293e-07 1.4284126e-01 1.4283411e-01 1.4281292e-01]]\n",
      "Step 1000, Loss: [-0.83677936] Probs: [[1.4279522e-01 1.0297455e-04 1.4285760e-01 1.4285040e-01 3.9628900e-05\n",
      "  1.4286652e-01 1.7786245e-07 1.4284325e-01 1.4283283e-01 1.4281145e-01]]\n",
      "Step 1200, Loss: [-0.83678067] Probs: [[1.4279492e-01 1.0515715e-04 1.4285935e-01 1.4285013e-01 3.8906455e-05\n",
      "  1.4286733e-01 1.7988268e-07 1.4284362e-01 1.4283183e-01 1.4280854e-01]]\n",
      "Step 1400, Loss: [-0.83678204] Probs: [[1.42792657e-01 1.07779815e-04 1.42857790e-01 1.42849997e-01\n",
      "  3.80245147e-05 1.42870471e-01 1.81744483e-07 1.42845675e-01\n",
      "  1.42830387e-01 1.42807052e-01]]\n",
      "Step 1600, Loss: [-0.83678395] Probs: [[1.4278965e-01 1.1108294e-04 1.4285822e-01 1.4285077e-01 3.6927180e-05\n",
      "  1.4287239e-01 1.8340960e-07 1.4284804e-01 1.4282842e-01 1.4280428e-01]]\n",
      "Step 1800, Loss: [-0.83678645] Probs: [[1.42787635e-01 1.15448034e-04 1.42855301e-01 1.42849460e-01\n",
      "  3.55261800e-05 1.42876640e-01 1.84761234e-07 1.42852232e-01\n",
      "  1.42825007e-01 1.42802507e-01]]\n",
      "Step 2000, Loss: [-0.8367899] Probs: [[1.4278230e-01 1.2143115e-04 1.4285742e-01 1.4285204e-01 3.3723842e-05\n",
      "  1.4287871e-01 1.8564350e-07 1.4285745e-01 1.4282048e-01 1.4279623e-01]]\n",
      "Step 2200, Loss: [-0.8367952] Probs: [[1.4277300e-01 1.3007683e-04 1.4286010e-01 1.4285067e-01 3.1423857e-05\n",
      "  1.4288579e-01 1.8633240e-07 1.4286985e-01 1.4281614e-01 1.4278281e-01]]\n",
      "Step 2400, Loss: [-0.8368026] Probs: [[1.4276035e-01 1.4143846e-04 1.4287581e-01 1.4284812e-01 2.8898652e-05\n",
      "  1.4288555e-01 1.8820032e-07 1.4290708e-01 1.4279860e-01 1.4275397e-01]]\n",
      "Step 2600, Loss: [-0.8368125] Probs: [[1.4276099e-01 1.5604879e-04 1.4291117e-01 1.4284772e-01 2.6268306e-05\n",
      "  1.4284053e-01 1.9177821e-07 1.4293632e-01 1.4278108e-01 1.4273970e-01]]\n",
      "Step 2800, Loss: [-0.8368298] Probs: [[1.4274511e-01 1.7998152e-04 1.4288029e-01 1.4283349e-01 2.3085513e-05\n",
      "  1.4290723e-01 2.0024014e-07 1.4298078e-01 1.4275666e-01 1.4269319e-01]]\n",
      "Step 3000, Loss: [-0.83607906] Probs: [[1.4353986e-01 2.1408481e-04 1.3296930e-01 1.4196731e-01 2.0449701e-05\n",
      "  1.5234962e-01 2.2902010e-07 1.3269088e-01 1.4986974e-01 1.4637856e-01]]\n",
      "Step 3200, Loss: [-0.83691484] Probs: [[1.4271632e-01 2.8967595e-04 1.4291687e-01 1.4280018e-01 1.6047259e-05\n",
      "  1.4294319e-01 2.9522184e-07 1.4308901e-01 1.4263420e-01 1.4259422e-01]]\n",
      "Step 3400, Loss: [-0.838086] Probs: [[1.4240848e-01 1.9436529e-03 1.4369172e-01 1.4152515e-01 1.3334658e-05\n",
      "  1.4596123e-01 7.2668872e-06 1.4136133e-01 1.4247724e-01 1.4061064e-01]]\n",
      "Step 3600, Loss: [-1.] Probs: [[0.10004526 0.10003132 0.09999195 0.09997809 0.09998549 0.10002945\n",
      "  0.10003029 0.09998844 0.09998028 0.09993944]]\n",
      "Step 3800, Loss: [-1.] Probs: [[0.10000008 0.10000005 0.09999997 0.09999996 0.09999995 0.09999994\n",
      "  0.10000007 0.09999988 0.10000014 0.09999997]]\n",
      "Step 4000, Loss: [-0.99999994] Probs: [[0.10000007 0.09999993 0.09999999 0.09999998 0.10000007 0.10000011\n",
      "  0.10000014 0.09999988 0.09999996 0.09999989]]\n",
      "Step 4200, Loss: [-1.0000001] Probs: [[0.10000017 0.09999994 0.09999989 0.10000008 0.0999999  0.09999996\n",
      "  0.09999969 0.09999993 0.10000025 0.1000002 ]]\n",
      "Step 4400, Loss: [-1.] Probs: [[0.09999991 0.10000014 0.10000011 0.09999997 0.09999982 0.09999996\n",
      "  0.09999995 0.10000005 0.10000014 0.09999998]]\n",
      "Step 4600, Loss: [-1.] Probs: [[0.10000008 0.09999982 0.09999994 0.10000011 0.10000013 0.09999991\n",
      "  0.09999959 0.10000022 0.10000001 0.10000025]]\n",
      "Step 4800, Loss: [-1.] Probs: [[0.09999991 0.1000001  0.10000001 0.1        0.09999992 0.10000005\n",
      "  0.10000001 0.10000001 0.10000001 0.09999994]]\n",
      "Step 5000, Loss: [-1.] Probs: [[0.10000004 0.09999987 0.09999992 0.10000005 0.10000006 0.09999996\n",
      "  0.09999986 0.1000001  0.10000003 0.1000001 ]]\n",
      "Step 5200, Loss: [-1.] Probs: [[0.1        0.10000008 0.10000005 0.09999998 0.09999993 0.10000011\n",
      "  0.10000032 0.09999971 0.10000002 0.0999998 ]]\n",
      "Step 5400, Loss: [-1.0000001] Probs: [[0.10000003 0.09999991 0.09999994 0.09999997 0.10000012 0.10000001\n",
      "  0.10000017 0.09999997 0.09999996 0.09999993]]\n",
      "Step 5600, Loss: [-1.] Probs: [[0.09999993 0.10000006 0.09999997 0.10000004 0.09999997 0.09999995\n",
      "  0.09999982 0.10000017 0.10000003 0.10000008]]\n",
      "Step 5800, Loss: [-1.] Probs: [[0.10000002 0.09999993 0.09999979 0.09999989 0.1000002  0.09999992\n",
      "  0.09999979 0.10000011 0.10000005 0.10000025]]\n",
      "Step 6000, Loss: [-1.] Probs: [[0.1000001  0.09999992 0.09999995 0.09999997 0.10000012 0.09999999\n",
      "  0.10000024 0.09999981 0.09999999 0.09999993]]\n",
      "Step 6200, Loss: [-1.] Probs: [[0.09999993 0.10000001 0.09999994 0.10000005 0.09999998 0.10000001\n",
      "  0.09999961 0.10000023 0.10000002 0.10000014]]\n",
      "Step 6400, Loss: [-1.] Probs: [[0.09999997 0.10000014 0.10000007 0.09999996 0.09999994 0.09999993\n",
      "  0.10000021 0.09999996 0.09999999 0.09999989]]\n",
      "Step 6600, Loss: [-1.0000001] Probs: [[0.10000014 0.09999996 0.09999984 0.09999999 0.09999996 0.10000002\n",
      "  0.09999981 0.09999995 0.10000021 0.10000014]]\n",
      "Step 6800, Loss: [-1.] Probs: [[0.1000002  0.0999999  0.09999993 0.0999998  0.10000017 0.1000001\n",
      "  0.10000088 0.09999946 0.09999986 0.09999971]]\n",
      "Step 7000, Loss: [-1.0000001] Probs: [[0.09999997 0.10000015 0.10000007 0.09999997 0.09999985 0.09999993\n",
      "  0.10000036 0.09999993 0.09999992 0.09999979]]\n",
      "Step 7200, Loss: [-1.] Probs: [[0.09999999 0.10000003 0.10000008 0.09999997 0.1        0.10000003\n",
      "  0.10000036 0.09999984 0.09999987 0.0999998 ]]\n",
      "Step 7400, Loss: [-1.] Probs: [[0.09999994 0.10000011 0.10000005 0.09999999 0.09999996 0.10000005\n",
      "  0.10000017 0.0999999  0.10000001 0.09999987]]\n",
      "Step 7600, Loss: [-1.0000001] Probs: [[0.10000002 0.09999985 0.09999993 0.09999997 0.10000015 0.09999999\n",
      "  0.0999999  0.10000002 0.09999999 0.10000013]]\n",
      "Step 7800, Loss: [-1.] Probs: [[0.10000008 0.09999986 0.10000002 0.09999996 0.1000001  0.09999998\n",
      "  0.09999999 0.10000002 0.09999996 0.1000001 ]]\n",
      "tf.Tensor(\n",
      "[[0.09999989 0.10000001 0.10000007 0.10000016 0.09999985 0.09999989\n",
      "  0.09999941 0.10000043 0.10000008 0.10000022]], shape=(1, 10), dtype=float32)\n",
      "Balanced Image:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-e34a0306fe83>:42: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 228 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  input_image[0, coord[0], coord[1], 0].assign(np.int8(int(hex_values[i], 16)))\n",
      "<ipython-input-46-e34a0306fe83>:42: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 216 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  input_image[0, coord[0], coord[1], 0].assign(np.int8(int(hex_values[i], 16)))\n",
      "<ipython-input-46-e34a0306fe83>:42: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 247 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  input_image[0, coord[0], coord[1], 0].assign(np.int8(int(hex_values[i], 16)))\n",
      "<ipython-input-46-e34a0306fe83>:42: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 174 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  input_image[0, coord[0], coord[1], 0].assign(np.int8(int(hex_values[i], 16)))\n",
      "<ipython-input-46-e34a0306fe83>:42: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 166 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  input_image[0, coord[0], coord[1], 0].assign(np.int8(int(hex_values[i], 16)))\n",
      "<ipython-input-46-e34a0306fe83>:42: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 200 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  input_image[0, coord[0], coord[1], 0].assign(np.int8(int(hex_values[i], 16)))\n",
      "<ipython-input-46-e34a0306fe83>:42: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 135 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  input_image[0, coord[0], coord[1], 0].assign(np.int8(int(hex_values[i], 16)))\n",
      "<ipython-input-46-e34a0306fe83>:42: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 246 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  input_image[0, coord[0], coord[1], 0].assign(np.int8(int(hex_values[i], 16)))\n",
      "<ipython-input-46-e34a0306fe83>:42: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 242 to int8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  input_image[0, coord[0], coord[1], 0].assign(np.int8(int(hex_values[i], 16)))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtwElEQVR4nO3deXDUdZrH8acJSXMlDQFykQDhvmFADBnlGMgQ8CiuHXF0q3BkYWCDCuiqmVVRZ5wgzrp4IJQlA0vJoVADLK4yA2iCIoEBQcCRCJFLIUGOdOe++rd/WGSMcnyfkOabhPerqqsg+fDk2+kkHzrpPO1yHMcRAABusEa2DwAAuDlRQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsaGz7AD/m9/vl9OnTEhoaKi6Xy/ZxAABKjuNIfn6+xMTESKNGV76fU+cK6PTp0xIXF2f7GACA63Tq1CmJjY294usDVkCLFi2Sl156SXJycqR///7y2muvya233nrNfxcaGioiImFhYcb3gDp16mR8roiICOOsiEhRUZFxtlu3bqrZwcHBxtkVK1aoZldWVhpnJ0yYoJq9YMECVd7j8RhnDx48qJr9X//1X8bZ999/XzW7oqLCOHu1/+Vdzm9/+1tVvkmTJsbZM2fOqGZ7vV7jbHh4uGr2yZMnjbNPP/20ava+ffuMs9OmTVPN1srPzzfObtq0STX74YcfNs5qv2uk+TqhyV5y6ev5lQSkgN555x2ZO3euLFmyRBISEmThwoWSnJwsWVlZ1yyAS+9Al8tl/M4MCgoyPpvmi76ISOPG5u+ikJAQ1WxNXvuBpclrzx0WFhawfIsWLVSzNbdnIN+H2tlutztgee3tqXkfamdrPn+0t33Tpk2Ns9qPWS3N7a85t3Z2ID/Ga+Ja8wPyIISXX35Zpk2bJr/5zW+kV69esmTJEmnWrJn8+c9/DsSbAwDUQ7VeQGVlZbJ3715JSkr65xtp1EiSkpJk586dP8mXlpaKz+erdgEANHy1XkDnzp2TyspKiYyMrPbyyMhIycnJ+Uk+LS1NPB5P1YUHIADAzcH67wGlpqaK1+utupw6dcr2kQAAN0CtPwihTZs2EhQUJLm5udVenpubK1FRUT/Ju91u9Q9kAQD1X63fAwoJCZFBgwbJtm3bql7m9/tl27ZtkpiYWNtvDgBQTwXkYdhz586VKVOmyC233CK33nqrLFy4UAoLC+U3v/lNIN4cAKAeCkgBTZ48Wb777jt55plnJCcnRwYMGCCbN2/+yQMTAAA3L5fjOI7tQ/yQz+cTj8cjSUlJxr8gp/ktcc0vrYqITJw40Tir3Shw9OhR42ybNm1UszUbH7S/xa9VUFBgnL3//vtVsy/3c8UrSU5OVs0ePXq0cVb7S5Ra69atM85qPw537NhhnN2wYYNq9rV+E/6H2rZtq5r9f//3f8bZF154QTV74MCBqvzWrVuNs6+//rpq9vbt242z2s9lzS+iXrx40TjrOI74/X7xer1X/SVg64+CAwDcnCggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVdXYVz7333mv8/PO7du0ynv+zn/1MdZ7Vq1cbZ48fP66aXV5ebpzt2rWrarbG4cOHVfnOnTur8qYrlUREKisrVbO1q5UC5e2331blO3bsqMoPGDDAOJuXl6eaHRsbq8oHSkVFhSqv+fy53JNhXs2hQ4dU+Ycfftg4q92JqVnZFcjPh6KiIuOs4zhSWFjIKh4AQN1EAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWNLZ9gCtZv369uFwuo2yXLl2M5x45ckR1Dr/fb5zV7jEL5H43jYMHD6ryERERqnx4eLhxNpC7rPbt26fKz5o1yzir3TWWnJysyickJBhn68puN63GjXVfjjT5+Ph41ex27dqp8jt27DDOZmZmqmZPmzbNONupUyfV7C+//NI4W1BQoJptgntAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUux3Ec24f4IZ/PJx6PRzwej/EqnpkzZxrPb9q0qeo8kZGRxtkHH3xQNVu7ekRj3bp1xtmkpCTV7JYtWypPY87r9aryL774onF27dq1qtkVFRXG2QsXLqhm5+fnq/Ka22jgwIGq2Zrbc+nSparZzZs3N84++uijqtkTJkwwzrZo0UI1+2bRu3dv42xYWJhxtqKiQvbs2SNer/eq/457QAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwIo6uwtu0qRJEhwcbPRvfvvb3xrPP3HihOo8//qv/2qcDQoKUs3WePvtt1X5AQMGGGf79OmjPI3O66+/bpxdvHixavbRo0eNs5rdbiIifr9flQ+kXr16GWcLCgpUs0+ePKk9TkDExcWp8po9jcuWLVPNDvTnRF2h+RjXVIXP55Pw8HB2wQEA6qZaL6Bnn31WXC5XtUuPHj1q+80AAOq5gDwfQO/evWXr1q3/fCMBfNoBAED9FJBmaNy4sURFRQViNACggQjIz4COHDkiMTEx0qlTJ7n//vuv+kPO0tJS8fl81S4AgIav1gsoISFBli9fLps3b5bFixfLsWPHZOjQoVd8Bsi0tLSqZ0D1eDzqR8IAAOqnWi+gsWPHyq9+9Svp16+fJCcny/vvvy95eXny7rvvXjafmpoqXq+36nLq1KnaPhIAoA4K+KMDWrZsKd26dbvi72u43W5xu92BPgYAoI4J+O8BFRQUSHZ2tkRHRwf6TQEA6pFaL6DHHntMMjIy5Pjx4/Lpp5/KhAkTJCgoSH7961/X9psCANRjtf4tuG+++UZ+/etfy/nz56Vt27Zy++23S2ZmprRt21Y1JywsTEJCQoyyXbp0MZ7bvHlz1Tk063XS09NVs//85z8bZ1955RXV7FatWqnyGiUlJaq8ZjVMdna2anZ5ebkqr+FyuQI2+5577lHlv/jiC+Os9ueomusZyM1dFy9eVOVNvz6IfP+zZo34+HhV/j//8z+Nsx6PRzW7tLTUOHv27FnV7E6dOhlnNWt7TLO1XkBr1qyp7ZEAgAaIXXAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFQF/Ooaa+p//+R/jHVU/+9nPjOdqdyVpnipi0aJFqtkrV640zmr2XonodqRlZWWpZm/YsEGVf/rpp42zgdy/pqXZe6Y99+eff67Ka/YdavbGiQR2v5tm92JUVJRqduPG5l++PvvsM9XsrVu3qvJvvvmmcXb48OGq2SkpKcbZbt26qWbv37/fOHvkyBHjbFFRkVGOe0AAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFS4nkHs4asDn84nH45Ho6Ghp1MisH/1+v/H8d955R3Wen//858ZZr9ermh0eHm6cLSsrU83esWOHcfaOO+5QzS4pKVHlA0mzAqcufaj/6le/UuWHDh1qnP32229VsxcsWGCcDQsLU81u166dcTY+Pl41OyIiwjj797//XTX71KlTqrxm9ZX2emq+Bh0/flw1e/fu3cZZzee94zhSVlYmXq/3qh8z3AMCAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWNLZ9gCtp3bq1BAUFGWUHDhxoPLdPnz6qc5ieoSY0u+OGDx8esHPUpd1uWnVpv5tG7969VfkhQ4YYZw8fPqya3b9/f+Osds+cZoehdv9aTk6OcfbcuXOq2YGUnZ0dsLxmL6aISOPG5hVQWlqqmm2Ce0AAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMCKOrsL7t/+7d+kadOmRtnIyEjjuWfPnlWdo1WrVsbZ8PBw1WzNbqULFy6oZmv2arlcLtXs+iqQe+Patm2ryo8ZM0aVHzx4sHE2JCRENfuWW24xzubm5qpma97nRUVFqtkVFRXG2fPnz6tml5eXq/IajRrp/t/vdruNs9rPZdu7FLkHBACwQl1A27dvl7vvvltiYmLE5XLJhg0bqr3ecRx55plnJDo6Wpo2bSpJSUly5MiR2jovAKCBUBdQYWGh9O/fXxYtWnTZ1y9YsEBeffVVWbJkiezatUuaN28uycnJ9XrlPwCg9ql/BjR27FgZO3bsZV/nOI4sXLhQnnrqKRk3bpyIiKxYsUIiIyNlw4YNcu+9917faQEADUat/gzo2LFjkpOTI0lJSVUv83g8kpCQIDt37rzsvyktLRWfz1ftAgBo+Gq1gC49Q+GPH5UWGRl5xWcvTEtLE4/HU3WJi4urzSMBAOoo64+CS01NFa/XW3XRPi0vAKB+qtUCioqKEpGf/q5Abm5u1et+zO12S1hYWLULAKDhq9UCio+Pl6ioKNm2bVvVy3w+n+zatUsSExNr800BAOo59aPgCgoK5OjRo1V/P3bsmOzfv1/Cw8Olffv2Mnv2bPnDH/4gXbt2lfj4eHn66aclJiZGxo8fX5vnBgDUc+oC2rNnj/ziF7+o+vvcuXNFRGTKlCmyfPlyefzxx6WwsFCmT58ueXl5cvvtt8vmzZulSZMmqrdTUVFhvGojLy/PeG58fLzqHBqffPKJKt+4sfm7/6677lLNXrdunXG2efPmqtnFxcWqvGb1yJAhQ1SzT58+bZytrKxUzW7Tpo1xdurUqarZCQkJqrzf7zfOrly5UjW7Z8+extnCwkLV7Cs9+vVytLePZl1OIFfraAUFBdk+QpWysjLj7G233WacraiokF27dl0zpy6gESNGXHV/kMvlkueff16ef/557WgAwE3E+qPgAAA3JwoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGCFehXPjbJ582bjXWmDBw82nhsSElLTI13T7bffrsoXFBQYZydOnKia3a9fP+Psz3/+c9Vsl8ulymt2X13piQuv5EpPD385c+bMUc0ePnx4QM5RE5p9ejNmzFDN/uijj4yzJvu9fkizA1Kzl0ykbu1Ua9WqlXE2NDRUNTs6Oto4u3btWtXsiIgI46xmV5/P5zM6N/eAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACvq7Cqe8vJycRzHKKtZr/Ppp5+qzqFZU+P3+1WzW7RoYZzt37+/ana3bt2Ms+3atVPN1q5AOXfunHF23bp1qtnTp083zk6YMEE1OyEhQZWvK5YsWaLKP/nkk8ZZ7RqZCxcuGGdPnjypmq25PbUrhLKzs1X5vLw842zHjh1VszWf+9rPZc2Kp+LiYuOs6bou7gEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArXI7pwrUbxOfzicfjEbfbbbxP6IUXXjCev3XrVtV5XnvtNeNs586dVbM1NLumtPk9e/aoZn/88ceqfFZWlnFWu09Pc3t27dpVNXvevHnG2V69eqlma/MlJSXG2SZNmqhma97n2i8XpaWlAcmKiDRr1sw4q/380b4P27Ztq8rXFZr9bmVlZcZZn88n7du3F6/XK2FhYVfMcQ8IAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsKKx7QNcydChQyU4ONgoO3fuXOO5mqzWhQsXVPk33njDOPvMM8+oZmvWyCxZskQ1e+PGjar8m2++aZzt3r27anZ4eLhxduXKlarZv/vd74yzd911l2r2oEGDVPmDBw8aZ7Urh3bs2GGc1a5KCgkJMc42aqT7/7DX6zXOZmRkqGZrb8+vv/7aONupUyfVbM2KoqKiItVszfuwvLzcOFtQUGCU4x4QAMAKCggAYIW6gLZv3y533323xMTEiMvlkg0bNlR7/QMPPCAul6vaZcyYMbV1XgBAA6EuoMLCQunfv78sWrToipkxY8bImTNnqi6rV6++rkMCABoe9YMQxo4dK2PHjr1qxu12S1RUVI0PBQBo+ALyM6D09HSJiIiQ7t27y8yZM+X8+fNXzJaWlorP56t2AQA0fLVeQGPGjJEVK1bItm3b5MUXX5SMjAwZO3asVFZWXjaflpYmHo+n6hIXF1fbRwIA1EG1/ntA9957b9Wf+/btK/369ZPOnTtLenq6jBo16if51NTUar+b4/P5KCEAuAkE/GHYnTp1kjZt2sjRo0cv+3q32y1hYWHVLgCAhi/gBfTNN9/I+fPnJTo6OtBvCgBQj6i/BVdQUFDt3syxY8dk//79Eh4eLuHh4fLcc8/JpEmTJCoqSrKzs+Xxxx+XLl26SHJycq0eHABQv7kcx3E0/yA9PV1+8Ytf/OTlU6ZMkcWLF8v48eNl3759kpeXJzExMTJ69Gj5/e9/L5GRkUbzfT6feDweGTp0qDRubNaPb7/9tvH5V6xYYZwVERk9erRxdsCAAarZmt1XxcXFqtmaHVyme5su+dOf/qTK/+EPfzDOar8Fq9l79stf/lI1+8477zTO3n777arZ2p1qFRUVxtnevXurZn/++efG2fnz56tmT5gwwTh7td8tvJznn3/eOHulB0FdSevWrVV5zS64qz0q+HKOHz9unPV4PKrZ+fn5xtk+ffoYZwsKCmTYsGHi9Xqv+jmtvgc0YsQIuVpn/fWvf9WOBADchNgFBwCwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwggICAFhR688HVFuaNWsmwcHBRtmePXsaz9Vu5b7//vuNs5rdblput1uV15wlKChINfvIkSOqvGYf2N69e1WzO3bsaJydOXOmanZsbKwqr/HVV1+p8t26dTPOvvHGG6rZOTk5xtlnn31WNVvjrbfeUuWLioqMs3v27FHNbtWqlSqvuX3KyspUs0+fPm2c1e68u+eee4yzmq8Tps9szT0gAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAqX4ziO7UP8kM/nE4/Ho/o3TzzxhHF2/vz52iMFjGYlx86dO1WzNStTIiMjVbMvXryoyoeFhRln27Vrp5q9cOFC46zf71fNPnHihHHWdPXIJfHx8aq8ZkXRpk2bVLP/9Kc/GWe166a+/PJL4+yDDz6omn3nnXcaZ7XriVatWqXKt2zZ0jjbuXNn1ezQ0FDj7KFDh1SzTdediYikpaUZZ8vKymT16tXi9Xqv+vnPPSAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADACgoIAGBFnd4F53K5jP7Nd999Zzy/devWqvNkZmYaZ4cPH66aXVFRYZzV3kyaHU+NGzdWzTa9XS4ZM2aMcfbdd99Vzda8X7Tn/uijj4yzISEhqtm7du1S5R9++GHjrHZf24oVK4yzBw8eVM3eunWrcfbo0aOq2W632zjr9XpVs7UfK4MHDzbOrl+/XjU7JiZGldfQfO3cuHGjcba4uFgefvhhdsEBAOomCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYIVuB8sNdK0VDjX15ptvqvJpaWnG2fLycu1xjGlXg2hW8WhWmtTE9u3bjbPaNTIXL140zu7evVs1++TJk8bZUaNGqWbPmDFDlf/mm2+Ms4WFharZmo8tzftbROTEiRPG2dLSUtXskpISVV5D+3E4adIk4+yFCxdUswO5ikezWmnevHnGWb/fb5TjHhAAwApVAaWlpcngwYMlNDRUIiIiZPz48ZKVlVUtU1JSIikpKdK6dWtp0aKFTJo0SXJzc2v10ACA+k9VQBkZGZKSkiKZmZmyZcsWKS8vl9GjR1e7yz9nzhzZtGmTrF27VjIyMuT06dMyceLEWj84AKB+U/0MaPPmzdX+vnz5comIiJC9e/fKsGHDxOv1ytKlS2XVqlUycuRIERFZtmyZ9OzZUzIzM2XIkCG1d3IAQL12XT8DuvQcG+Hh4SIisnfvXikvL5ekpKSqTI8ePaR9+/ayc+fOy84oLS0Vn89X7QIAaPhqXEB+v19mz54tt912m/Tp00dERHJyciQkJERatmxZLRsZGSk5OTmXnZOWliYej6fqEhcXV9MjAQDqkRoXUEpKihw6dEjWrFlzXQdITU0Vr9dbdTl16tR1zQMA1A81+j2gWbNmyXvvvSfbt2+X2NjYqpdHRUVJWVmZ5OXlVbsXlJubK1FRUZed5Xa7A/57KACAukd1D8hxHJk1a5asX79ePvzwQ4mPj6/2+kGDBklwcLBs27at6mVZWVly8uRJSUxMrJ0TAwAaBNU9oJSUFFm1apVs3LhRQkNDq36u4/F4pGnTpuLxeGTq1Kkyd+5cCQ8Pl7CwMHnooYckMTGRR8ABAKpRFdDixYtFRGTEiBHVXr5s2TJ54IEHRETkv//7v6VRo0YyadIkKS0tleTkZHnjjTdq5bAAgIbD5TiOY/sQP+Tz+cTj8QRsF9yBAwdU+UceecQ4++mnn6pmV1RUGGeDgoJUs0NCQoyzTZo0Uc3u0aOHKp+cnGyc/fG3da/lq6++Ms5269ZNNfvH/9G6mh/+LNSE9uPw0KFDxtlLvx5hSrMP7G9/+5tq9tdff63KawTyS9cf//hHVb5nz57G2fHjx6tmaz5/du3apZodKI7jiM/nu+bXcXbBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFbU6OkYboSioiJp3NjseJpVMhEREapzaNYBtWrVSjVbs4pHkxXRre4xfT9fcqWn1riS7t27G2c7dOigmq1Z9ZKXl6ea/e233xpnjx49qprdvn17Vf7ChQvG2Y8//lg1+4MPPjDO5ufnq2YHUmhoqHH2xIkTqtnaz2UNzW0polt/pFnBJfL9E4ua0n4NMsE9IACAFRQQAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYEWd3QXXrFkzadasmVFWs88oODhYdY5hw4YZZ91ut2q2ZjeZy+VSzfZ6vcZZ0/fzJbGxsar8gAEDjLPavXSZmZnG2aFDh6pmJyQkGGcPHjyomt22bVtVfsuWLcbZ5s2bq2bXlf1u2j2Ay5YtM84GcrebiMiePXuMsy+//HLAzlFWVhaw2YHAPSAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBUUEADAijq7iuett96Spk2bGmVnzpxpPLd169aqczz66KOqPH5Ks+pFu0qkc+fOxtlvv/1WNXvp0qXG2a5du6pmf/fdd6p8WFiYcVb7PgwKCjLOjh49WjVbs85o3rx5qtl1iWa9zr59+1SzPR6PcVazgkvrqaeeMs6WlpbKSy+9dM0c94AAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVLsdxHNuH+CGfz1e1+8jlchn9G83+o9DQ0BqdqyErLS1V5UtKSlR5v99vnD158qRq9pgxY4yzlZWVqtnFxcUBmz1y5EhVXjNfszdOROSrr74yzv7+979Xzb7rrrtU+UDRfAzWxH333WecPXTokGr24cOHjbO7d+9Wze7Zs6dx1nQ3p8g/v457vd6rfjxyDwgAYIWqgNLS0mTw4MESGhoqERERMn78eMnKyqqWGTFihLhcrmqXGTNm1OqhAQD1n6qAMjIyJCUlRTIzM2XLli1SXl4uo0ePlsLCwmq5adOmyZkzZ6ouCxYsqNVDAwDqP9XzAW3evLna35cvXy4RERGyd+9eGTZsWNXLmzVrJlFRUbVzQgBAg3RdPwO69MP/8PDwai9fuXKltGnTRvr06SOpqalSVFR0xRmlpaXi8/mqXQAADV+NnxHV7/fL7Nmz5bbbbpM+ffpUvfy+++6TDh06SExMjBw4cECeeOIJycrKkr/85S+XnZOWlibPPfdcTY8BAKinalxAKSkpcujQIfnkk0+qvXz69OlVf+7bt69ER0fLqFGjJDs7+7JPn5yamipz586t+rvP55O4uLiaHgsAUE/UqIBmzZol7733nmzfvl1iY2Ovmr30nPBHjx69bAG53W5xu901OQYAoB5TFZDjOPLQQw/J+vXrJT09XeLj46/5b/bv3y8iItHR0TU6IACgYVIVUEpKiqxatUo2btwooaGhkpOTIyIiHo9HmjZtKtnZ2bJq1Sq54447pHXr1nLgwAGZM2eODBs2TPr16xeQKwAAqJ9UBbR48WIR+f6XTX9o2bJl8sADD0hISIhs3bpVFi5cKIWFhRIXFyeTJk2Sp556qtYODABoGNTfgruauLg4ycjIuK4Dad7eJdOmTTOeOWjQINUZTPfRiYgMHjxYNftyPxO7kosXL6pmB3I/Xn5+virfuLH5h9nnn3+umt2mTRvjrGbnmYhu/5p219j777+vys+ePds4O3DgQNXsyZMnG2eDg4NVszUqKipUec3H4auvvqqarX0frlmzxjir2TEoIrJ9+3bjbPPmzVWzNfvdAoFdcAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVNX4+oEC7tFvOhGbNxv/+7/+qzqFZaXPkyBHV7C1bthhnT58+rZp9yy23GGdbt26tmt2qVStVfuTIkcbZ48ePq2Z/9913xtmysjLV7EAaMmSIKv/CCy8YZ01XWF2iWa9TWlqqml1SUmKcTUlJUc3+4XOPXcvEiRNVs/v27avKa1ztGaIvJyYmxjjbrl077XGs4h4QAMAKCggAYAUFBACwggICAFhBAQEArKCAAABWUEAAACsoIACAFRQQAMAKCggAYAUFBACwos7uguvSpYs0adLEKFteXm48V7OXTESkadOmxlm/36+ardmTpcmKiDRqZP5/C+1uN5fLpcpXVFQYZ++55x7V7EmTJhlnNR8nIvrrqaH5uAo0zY681NTUgJ3j888/V+Vff/114+yMGTNUsz/77DNVXrMHct26darZhw4dUuU1/uVf/sU4O3z4cONsYWGhUY57QAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFZQQAAAKyggAIAVLsdxHNuH+CGfzycej0deffVV43UlxcXFxvO7d++uOk9ERIRxNjQ0VDW7oKDAOHv48GHV7JCQEOPsiBEjVLNLS0tVec3aGc3aHi3tuTW3p3a1TuPGgduCpX0f7ty50zg7YcIE1ewhQ4YYZ01Xb10SGxtrnP3HP/6hmu12u1X5Xr16GWdbt26tmu3xeIyzOTk5qtl33nmncXbfvn3G2eLiYnnkkUfE6/VKWFjYFXPcAwIAWEEBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsoIAAAFYEbiHVdcrNzTXeDdWjRw/juYsXL1ado2PHjsbZ4OBg1ewuXboYZzXXUUTkwoULxtkdO3aoZr/44ouq/B133GGc/fjjj1WzDx48aJzV3vZPPvmkcVa7B1C79+z48ePG2ZYtW6pmaz5WHn/8cdXsV155xTjbqJHu/8Pp6enGWe3uvdzcXFX+3LlzxlntvrZ27doZZz/99FPV7DVr1hhntfsoTXAPCABghaqAFi9eLP369ZOwsDAJCwuTxMRE+eCDD6peX1JSIikpKdK6dWtp0aKFTJo0Sf0/CQDAzUFVQLGxsTJ//nzZu3ev7NmzR0aOHCnjxo2TL774QkRE5syZI5s2bZK1a9dKRkaGnD59WiZOnBiQgwMA6jfVN0bvvvvuan9/4YUXZPHixZKZmSmxsbGydOlSWbVqlYwcOVJERJYtWyY9e/aUzMxM1fOCAAAavhr/DKiyslLWrFkjhYWFkpiYKHv37pXy8nJJSkqqyvTo0UPat29/1Se8Ki0tFZ/PV+0CAGj41AV08OBBadGihbjdbpkxY4asX79eevXqJTk5ORISEvKTR+BERkZe9VEfaWlp4vF4qi5xcXHqKwEAqH/UBdS9e3fZv3+/7Nq1S2bOnClTpkxRP93tD6WmporX6626nDp1qsazAAD1h/r3gEJCQqp+f2XQoEHy97//XV555RWZPHmylJWVSV5eXrV7Qbm5uRIVFXXFeW63W/386wCA+u+6fw/I7/dLaWmpDBo0SIKDg2Xbtm1Vr8vKypKTJ09KYmLi9b4ZAEADo7oHlJqaKmPHjpX27dtLfn6+rFq1StLT0+Wvf/2reDwemTp1qsydO1fCw8MlLCxMHnroIUlMTOQRcACAn3A5juOYhqdOnSrbtm2TM2fOiMfjkX79+skTTzwhv/zlL0Xk+19EffTRR2X16tVSWloqycnJ8sYbb1z1W3A/5vP5xOPxyGOPPWb8rTm/36+ar9G3b1/jbHl5uWp2aWmpcba4uFg1+/bbbw/IOUREOnfurMrfddddxtn77rtPNfvrr782zmrX5eTl5Rlnd+/erZp9/vx5VV7zcdu8eXPVbM3qHs1KIK2goCBVXvOApTNnzqhmaz8ncHler1fCwsKu+HrVPaClS5de9fVNmjSRRYsWyaJFizRjAQA3IXbBAQCsoIAAAFZQQAAAKyggAIAVFBAAwAoKCABgBQUEALCCAgIAWEEBAQCsUG/DDrRLm4E0qzA0q3jKyspU59GswNGu4tGcpaSkRDW7sLDQOKtdO5Kfn6/KV1ZWGme111PzPtTe9pq85jqK6D5mRf75eVHb2ZqcJVACeW7tbNSOa73fVbvgboRvvvmGJ6UDgAbg1KlTEhsbe8XX17kC8vv9cvr0aQkNDRWXy1X1cp/PJ3FxcXLq1KmrLrer77ieDcfNcB1FuJ4NTW1cT8dxJD8/X2JiYqRRoyv/pKfOfQuuUaNGV23MsLCwBn3jX8L1bDhuhusowvVsaK73eno8nmtmeBACAMAKCggAYEW9KSC32y3z5s0zfpK6+orr2XDcDNdRhOvZ0NzI61nnHoQAALg51Jt7QACAhoUCAgBYQQEBAKyggAAAVtSbAlq0aJF07NhRmjRpIgkJCbJ7927bR6pVzz77rLhcrmqXHj162D7Wddm+fbvcfffdEhMTIy6XSzZs2FDt9Y7jyDPPPCPR0dHStGlTSUpKkiNHjtg57HW41vV84IEHfnLbjhkzxs5haygtLU0GDx4soaGhEhERIePHj5esrKxqmZKSEklJSZHWrVtLixYtZNKkSZKbm2vpxDVjcj1HjBjxk9tzxowZlk5cM4sXL5Z+/fpV/bJpYmKifPDBB1Wvv1G3Zb0ooHfeeUfmzp0r8+bNk88++0z69+8vycnJcvbsWdtHq1W9e/eWM2fOVF0++eQT20e6LoWFhdK/f39ZtGjRZV+/YMECefXVV2XJkiWya9cuad68uSQnJ6sXktp2respIjJmzJhqt+3q1atv4AmvX0ZGhqSkpEhmZqZs2bJFysvLZfTo0dWW3s6ZM0c2bdoka9eulYyMDDl9+rRMnDjR4qn1TK6niMi0adOq3Z4LFiywdOKaiY2Nlfnz58vevXtlz549MnLkSBk3bpx88cUXInIDb0unHrj11ludlJSUqr9XVlY6MTExTlpamsVT1a558+Y5/fv3t32MgBERZ/369VV/9/v9TlRUlPPSSy9VvSwvL89xu93O6tWrLZywdvz4ejqO40yZMsUZN26clfMEytmzZx0RcTIyMhzH+f62Cw4OdtauXVuV+fLLLx0RcXbu3GnrmNftx9fTcRxn+PDhziOPPGLvUAHSqlUr56233rqht2WdvwdUVlYme/fulaSkpKqXNWrUSJKSkmTnzp0WT1b7jhw5IjExMdKpUye5//775eTJk7aPFDDHjh2TnJycarerx+ORhISEBne7ioikp6dLRESEdO/eXWbOnCnnz5+3faTr4vV6RUQkPDxcRET27t0r5eXl1W7PHj16SPv27ev17fnj63nJypUrpU2bNtKnTx9JTU2VoqIiG8erFZWVlbJmzRopLCyUxMTEG3pb1rllpD927tw5qayslMjIyGovj4yMlMOHD1s6Ve1LSEiQ5cuXS/fu3eXMmTPy3HPPydChQ+XQoUMSGhpq+3i1LicnR0Tksrfrpdc1FGPGjJGJEydKfHy8ZGdny+9+9zsZO3as7Ny5U4KCgmwfT83v98vs2bPltttukz59+ojI97dnSEiItGzZslq2Pt+el7ueIiL33XefdOjQQWJiYuTAgQPyxBNPSFZWlvzlL3+xeFq9gwcPSmJiopSUlEiLFi1k/fr10qtXL9m/f/8Nuy3rfAHdLMaOHVv15379+klCQoJ06NBB3n33XZk6darFk+F63XvvvVV/7tu3r/Tr1086d+4s6enpMmrUKIsnq5mUlBQ5dOhQvf8Z5bVc6XpOnz696s99+/aV6OhoGTVqlGRnZ0vnzp1v9DFrrHv37rJ//37xer2ybt06mTJlimRkZNzQM9T5b8G1adNGgoKCfvIIjNzcXImKirJ0qsBr2bKldOvWTY4ePWr7KAFx6ba72W5XEZFOnTpJmzZt6uVtO2vWLHnvvffko48+qva0KVFRUVJWViZ5eXnV8vX19rzS9bychIQEEZF6d3uGhIRIly5dZNCgQZKWlib9+/eXV1555YbelnW+gEJCQmTQoEGybdu2qpf5/X7Ztm2bJCYmWjxZYBUUFEh2drZER0fbPkpAxMfHS1RUVLXb1efzya5duxr07Sry/bP+nj9/vl7dto7jyKxZs2T9+vXy4YcfSnx8fLXXDxo0SIKDg6vdnllZWXLy5Ml6dXte63pezv79+0VE6tXteTl+v19KS0tv7G1Zqw9pCJA1a9Y4brfbWb58ufOPf/zDmT59utOyZUsnJyfH9tFqzaOPPuqkp6c7x44dc3bs2OEkJSU5bdq0cc6ePWv7aDWWn5/v7Nu3z9m3b58jIs7LL7/s7Nu3zzlx4oTjOI4zf/58p2XLls7GjRudAwcOOOPGjXPi4+Od4uJiyyfXudr1zM/Pdx577DFn586dzrFjx5ytW7c6AwcOdLp27eqUlJTYPrqxmTNnOh6Px0lPT3fOnDlTdSkqKqrKzJgxw2nfvr3z4YcfOnv27HESExOdxMREi6fWu9b1PHr0qPP88887e/bscY4dO+Zs3LjR6dSpkzNs2DDLJ9d58sknnYyMDOfYsWPOgQMHnCeffNJxuVzO3/72N8dxbtxtWS8KyHEc57XXXnPat2/vhISEOLfeequTmZlp+0i1avLkyU50dLQTEhLitGvXzpk8ebJz9OhR28e6Lh999JEjIj+5TJkyxXGc7x+K/fTTTzuRkZGO2+12Ro0a5WRlZdk9dA1c7XoWFRU5o0ePdtq2besEBwc7HTp0cKZNm1bv/vN0uesnIs6yZcuqMsXFxc6///u/O61atXKaNWvmTJgwwTlz5oy9Q9fAta7nyZMnnWHDhjnh4eGO2+12unTp4vzHf/yH4/V67R5c6cEHH3Q6dOjghISEOG3btnVGjRpVVT6Oc+NuS56OAQBgRZ3/GRAAoGGigAAAVlBAAAArKCAAgBUUEADACgoIAGAFBQQAsIICAgBYQQEBAKyggAAAVlBAAAArKCAAgBX/D+TW6uLNr4xNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------- Quantized model inference -------------\n",
    "interpreter = tf.lite.Interpreter(model_path=\"lenet5_mnist.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "in_scale, in_zero_point = interpreter.get_input_details()[0][\"quantization\"]\n",
    "# ---------- END Quantized model inference -----------\n",
    "# --------- NOT Quantized model inference ------------\n",
    "\"\"\"\n",
    "model = tf.keras.models.load_model(\"lenet5.keras\", custom_objects={\"Lenet\": Lenet})\n",
    "in_zero_point = 128\n",
    "in_scale = 0.003921568859368563\n",
    "\"\"\"\n",
    "# -------- END NOT Quantized model inference ---------\n",
    "\n",
    "input_mask = tf.Variable(tf.ones((1, 32, 32, 1)))\n",
    "coords = [(15,9),(15,10),(15,11),(15,12),(15,13),(15,14),(15,15),(15,16),(15,17),(15,18),(14,10),(13,11),(12,12),(11,18),(12,18),(13,18),(14,18),(16,18),(17,18),(18,18),(19,18)]\n",
    "for coord in coords:\n",
    "  input_mask[0, coord[0], coord[1], 0].assign(0)\n",
    "\n",
    "# Maintain constant values as f32 for the given coordinates (containing patterns)\n",
    "def apply_constants(image):\n",
    "  coords = [(15,9),(15,10),(15,11),(15,12),(15,13),(15,14),(15,15),(15,16),(15,17),(15,18),(14,10),(13,11),(12,12),(11,18),(12,18),(13,18),(14,18),(16,18),(17,18),(18,18),(19,18)]\n",
    "  hex_values = ['49', 'e4', '0a', 'd8', '7c', 'f7', '71', 'ae', '7e', '36', 'c8', '01', '87', 'f6', '5d', 'f6', '79', 'a6', '3a', 'f2', '7b']\n",
    "  # Convert the hex values to a numerical representation (float32)\n",
    "  float32_values = np.array([(np.int8(int(val, 16)) - in_zero_point) * in_scale for val in hex_values], dtype=np.float32)\n",
    "\n",
    "  # Assign the float32 values to the specified coordinates\n",
    "  for i, coord in enumerate(coords):\n",
    "    input_image[0, coord[0], coord[1], 0].assign(float32_values[i])\n",
    "\n",
    "# Maintain constant values as int8 for the given coordinates (containing patterns)\n",
    "def apply_constants_int8(image):\n",
    "  coords = [(15,9),(15,10),(15,11),(15,12),(15,13),(15,14),(15,15),(15,16),(15,17),(15,18),(14,10),(13,11),(12,12),(11,18),(12,18),(13,18),(14,18),(16,18),(17,18),(18,18),(19,18)]\n",
    "  hex_values = ['49', 'e4', '0a', 'd8', '7c', 'f7', '71', 'ae', '7e', '36', 'c8', '01', '87', 'f6', '5d', 'f6', '79', 'a6', '3a', 'f2', '7b']\n",
    "  for i, coord in enumerate(coords):\n",
    "    input_image[0, coord[0], coord[1], 0].assign(np.int8(int(hex_values[i], 16)))\n",
    "\n",
    "# Define the loss function: we want to minimize the variance of the predicted probabilities\n",
    "def loss_function(image):\n",
    "    logits = model(image)\n",
    "    target = tf.ones_like(logits) * 0.1\n",
    "    return tf.losses.cosine_similarity(target, logits)\n",
    "\n",
    "#------------------------------------------------------------------------#\n",
    "\n",
    "# Load the pre-trained model running the corresponding code snippet above\n",
    "\n",
    "# Initialize a random image with the shape expected by the model (e.g., 224x224x3 for a typical image model)\n",
    "input_image = tf.Variable(tf.random.uniform((1,32,32,1)), dtype=tf.float32)\n",
    "\n",
    "# Define the optimizer\n",
    "learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(.1, 5000, .1)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(8000):  # Number of optimization steps\n",
    "    with tf.GradientTape() as tape:\n",
    "      tape.watch(input_image)\n",
    "      input_image.assign(tf.clip_by_value(input_image, 0.0, 1.0))\n",
    "      apply_constants(input_image)\n",
    "      loss = loss_function(input_image)\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, [input_image])\n",
    "    # Apply gradients to the input image\n",
    "    optimizer.apply_gradients(zip(gradients, [input_image]))\n",
    "    if step % 200 == 0:\n",
    "      print(f\"Step {step}, Loss: {loss.numpy()} Probs: {model(input_image)}\")\n",
    "\n",
    "apply_constants(input_image)\n",
    "print(model(input_image))\n",
    "\n",
    "# numpy image as tensor\n",
    "balanced_image = input_image.numpy().reshape((1, 32, 32, 1))\n",
    "np.save(\"input_image_float32.npy\", balanced_image)\n",
    "\n",
    "# image as png image\n",
    "tf.keras.utils.save_img('input_image_class_vis.png', balanced_image, color_mode='grayscale')\n",
    "\n",
    "# image\n",
    "# Normalize the image values to be between 0 and 255\n",
    "balanced_image = balanced_image / in_scale + in_zero_point\n",
    "# force patterns to maintain their value, no quantization needed\n",
    "apply_constants_int8(balanced_image)\n",
    "balanced_image = balanced_image.astype(np.int8)\n",
    "print(\"Balanced Image:\")\n",
    "# Convert the numpy array to a PIL image\n",
    "np.save(\"input_image_int8.npy\", balanced_image)\n",
    "pil_image = tf.keras.preprocessing.image.array_to_img(balanced_image)\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(pil_image, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Convert the image to bytes\n",
    "#balanced_image_int8 = balanced_image.astype(np.int8)\n",
    "image_bytes = bytes(balanced_image)\n",
    "\n",
    "# Convert the bytes to hex\n",
    "image_hex = ' '.join(format(byte, '02x') for byte in image_bytes)\n",
    "\n",
    "with open('input_image_class_visualization.hex', 'w') as f:\n",
    "    f.write(image_hex)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "MtJz7wpnMvIn",
    "JTuveg1IzdQL",
    "IyEWUuLMlg6D",
    "zL5LZYE7ljqh",
    "DmwsPjlmt_Wn"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
